{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "\n",
    "This ipython notebook implements a simple Neural Nets aimed at classifying the MNIST dataset\n",
    "\n",
    "This code is mostly aimed for those who are familiar with the basics of Neural Networks, and want to see and understand an actual practical implementation.\n",
    "\n",
    "As such, this code doesn't use any higher level libraries such as Tensorflow. (It only makes extensive use of Numpy.)\n",
    "\n",
    "It also means that it is much easier to follow variables around, and see how different building blocks interact with each other to form a full-fledged net. This all comes at the drawback of not being able to use very optimized implementations.\n",
    "\n",
    "Therefore, although the presented net seems quite robust, it is also hard to optimize and converge, and have very good performance. Again, this code is meant for understanding and 'playing around', not aimed at high-end performance.\n",
    "\n",
    "<br />\n",
    "\n",
    "### I hope this gives an easy to follow code and a good overview of what happens inside a Neural Nets step by step, and that many people may benefit from tinkering with it.\n",
    "\n",
    "## Anner\n",
    "\n",
    "<br /> <br />\n",
    "\n",
    "NB:\n",
    "* Everything is included in this single iPython Notebook, EXCEPT:\n",
    "* The MNIST data. Please download these into the correct directory (explanation below)\n",
    "\n",
    "\n",
    "Lacking:\n",
    "\n",
    "* ReLU layer, other activation functions\n",
    "* Optimized initialisation of the weight variables such as Glorot/Xavier initialisation\n",
    "* Visualize different nets training characteristics next to each other\n",
    "* Conv nets are actually included in the building blocks, but not used. A new network class could add these layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries & building blocks\n",
    "### This is rather long, best to fold it and go over specific blocks once you feel like it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################## Neural Network Hulp/Utility functions ##################################\n",
    "# Here you can find a lot of building blocks for Neural Nets, sorry for not providing more detailed explanations\n",
    "# Some of these were build based on, or even copied from, my implementation for Stanford's cs231n class's homework assignments\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import struct\n",
    "import copy\n",
    "\n",
    "###################### Affine layer ########################\n",
    "\n",
    "def affine_layer_forward(inp, W, b):\n",
    "    # implements vanilla foward neural net layer, assumes all input to be numpy\n",
    "    out   = np.add(np.matmul(inp, W), b)\n",
    "    cache = (inp, W)\n",
    "    return out, cache\n",
    "\n",
    "def affine_layer_backward(dout, cache):\n",
    "    # implements derivative of vanilla foward neural net layer, assumes all input to be numpy\n",
    "    inp, W = cache\n",
    "    db     = np.sum(dout, axis=0)\n",
    "    dW     = np.dot(inp.T, dout)\n",
    "    dinp   = np.dot(dout, W.T)\n",
    "    return dinp, dW, db\n",
    "\n",
    "###################### Activation functions ################\n",
    "\n",
    "def sigmoid_forward(un_act):\n",
    "    # implements sigmoid activation function, assumes all input to be numpy\n",
    "    out   = 1 / (1 + np.exp(-un_act))\n",
    "    cache = un_act\n",
    "    return out, cache\n",
    "\n",
    "def sigmoid_backward(dout, cache):\n",
    "    # implements derivative of sigmoid acitvation function\n",
    "    un_act  = cache\n",
    "    dsig, _ = sigmoid_forward(un_act) #- np.power(sigmoid_forward(un_act), 2)\n",
    "    dun_act = np.multiply(dsig, dout)\n",
    "    return dun_act\n",
    "\n",
    "def softmax_forward(un_act):\n",
    "    # implements softmax for prediction layer, assumes all input to be numpy\n",
    "    # (no backward cause only used for last layer in combination with Cross Entropy Loss)\n",
    "    exp   = np.exp(un_act)\n",
    "    e_sum = np.sum(exp, axis=1, keepdims=True)\n",
    "    out   = exp/e_sum\n",
    "    return out\n",
    "\n",
    "###################### 'Add-ons'  ##########################\n",
    "\n",
    "def dropout_forward(inp, p):\n",
    "    # implements dropout function, assumes all input to be numpy\n",
    "    #out   = 1 / (1 + np.exp(-un_act))\n",
    "    mask  = None\n",
    "    mask  = np.random.rand(*inp.shape) > p\n",
    "    out   = np.multiply(mask, inp) / (1 - p)\n",
    "    cache = (p, mask)\n",
    "    return out, cache\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    # implements derivative of dropout function\n",
    "    p, mask = cache\n",
    "    dinp = np.multiply(dout, mask) / (1 - p)\n",
    "    return dinp\n",
    "\n",
    "def batch_norm_forward(inp, gamma, beta, bn_param):\n",
    "    # implements batch normalization function, assumes all input to be numpy\n",
    "    mode = bn_param['mode']\n",
    "    eps = bn_param.get('eps', 1e-5)\n",
    "    momentum = bn_param.get('momentum', 0.9)\n",
    "\n",
    "    N, D = inp.shape\n",
    "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=inp.dtype))\n",
    "    running_var = bn_param.get('running_var', np.zeros(D, dtype=inp.dtype))\n",
    "\n",
    "    out, cache = None, None\n",
    "    if mode == 'train':\n",
    "\n",
    "        inp_mean = np.sum(inp, axis=0) / N\n",
    "\n",
    "        inp_susq = np.sum(np.power(inp, 2), axis=0)\n",
    "        inp_sqsu = np.power(np.sum(inp, axis=0), 2)\n",
    "        inp_var = inp_susq / N - inp_sqsu / (N * N)\n",
    "        inp_std = np.sqrt(inp_var + eps)\n",
    "\n",
    "        inp_nor = (inp - inp_mean) / inp_std\n",
    "\n",
    "        out = gamma * inp_nor + beta\n",
    "\n",
    "        running_mean = momentum * running_mean + (1 - momentum) * inp_mean\n",
    "        running_var  = momentum * running_var + (1 - momentum) * inp_var\n",
    "\n",
    "        cache = (inp, inp_std, inp_nor, gamma, eps)\n",
    "\n",
    "    else:\n",
    "\n",
    "        inp_mean = (inp - running_mean)\n",
    "        inp_std = np.sqrt(running_var + eps)\n",
    "        inp_nor = inp_mean / inp_std\n",
    "\n",
    "        out = gamma * inp_nor + beta\n",
    "\n",
    "    # Store the updated running means back into bn_param\n",
    "    bn_param['running_mean'] = running_mean\n",
    "    bn_param['running_var'] = running_var\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "def batch_norm_backward(dout, cache):\n",
    "    # implements derivative of batch normalization function\n",
    "\n",
    "    dinp, dgamma, dbeta = None, None, None\n",
    "    inp, inp_std, inp_nor, gamma, eps = cache\n",
    "    inp_mu = np.mean(inp, axis=0)\n",
    "    N, D = inp.shape\n",
    "\n",
    "    dinp_nor = np.multiply(dout, gamma)\n",
    "\n",
    "    dinp = (1. / inp_std) * (\n",
    "    dinp_nor - (np.sum(dinp_nor, axis=0) / N) - (inp - inp_mu) * np.sum((inp - inp_mu) * dinp_nor, axis=0) * (inp_std ** (-2.) / N))\n",
    "    dgamma = np.sum(np.multiply(dout, inp_nor), axis=0)\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "\n",
    "    return dinp, dgamma, dbeta\n",
    "\n",
    "###################### Often used layer combinations  ######\n",
    "\n",
    "def sigmoid_layer_forward(inp, W, b):\n",
    "    # implements vanilla foward neural net layer, plus a sigmoid activation, assumes all input to be numpy\n",
    "    un_act, fc_cache  = affine_layer_forward(inp, W, b)\n",
    "    out,    sig_cache = sigmoid_forward(un_act)\n",
    "    return out, (fc_cache, sig_cache)\n",
    "\n",
    "def sigmoid_layer_backward(dout, cache):\n",
    "    # implements derivatives of vanilla foward neural net layer, plus a sigmoid activation, assumes all input to be numpy\n",
    "    fc_cache, sig_cache = cache\n",
    "    dun_act      = sigmoid_backward(dout, sig_cache)\n",
    "    dinp, dW, db = affine_layer_backward(dun_act, fc_cache)\n",
    "    return dinp, dW, db\n",
    "\n",
    "def BN_Dr_sig_layer_forward(inp, W, b, drop, gam, bet, bn_param):\n",
    "    # implements vanilla foward neural net layer, then BN, then Dropout, then a sigmoid activation, assumes all input to be numpy\n",
    "    un_act, fc_cache  = affine_layer_forward(inp, W, b)\n",
    "    BN,     BN_cache  = batch_norm_forward(un_act, gam, bet, bn_param)\n",
    "    Drop,   Dr_cache  = dropout_forward(BN, drop)\n",
    "    out,    sig_cache = sigmoid_forward(Drop)\n",
    "    return out, (fc_cache, BN_cache, Dr_cache, sig_cache)\n",
    "\n",
    "def BN_Dr_sig_layer_backward(dout, cache):\n",
    "    # implements derivatives of vanilla foward neural net layer, then BN, then Dropout, then a sigmoid activation, assumes all input to be numpy\n",
    "    fc_cache, BN_cache, Dr_cache, sig_cache = cache\n",
    "    dun_act         = sigmoid_backward(dout, sig_cache)\n",
    "    dDrop           = dropout_backward(dun_act, Dr_cache)\n",
    "    dBN, dgam, dbet = batch_norm_backward(dDrop, BN_cache)\n",
    "    dinp, dW, db    = affine_layer_backward(dBN, fc_cache)\n",
    "    return dinp, dW, db, dgam, dbet\n",
    "\n",
    "def softmax_layer_forward(inp, W, b):\n",
    "    # implements vanilla foward neural net layer, plus a softmax, for final prediction layer, assumes all input to be numpy\n",
    "    un_act, cache = affine_layer_forward(inp, W, b)\n",
    "    out           = softmax_forward(un_act)\n",
    "    return out, cache\n",
    "\n",
    "###################### Loss function #######################\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_gt):\n",
    "    # implements cross entropy loss and its derivative, assumes y as one hot encoded\n",
    "    loss   = np.mean(np.sum((y_gt * -np.log(y_pred)), axis=1))\n",
    "    d_pred = np.subtract(y_pred, y_gt)\n",
    "    return loss, d_pred\n",
    "\n",
    "###################### other ###############################\n",
    "\n",
    "def get_random_batch(X, y, batch_size):\n",
    "    # get a random batch from X and y for network training\n",
    "    N      = X.shape[0]\n",
    "    idx    = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    ba_idx = idx[:batch_size]\n",
    "    X_ba, y_ba = X[ba_idx], y[ba_idx]\n",
    "\n",
    "    return X_ba, y_ba\n",
    "\n",
    "###################### Convolutional layer #################\n",
    "\n",
    "def conv_layer_forward(inp, W, b):\n",
    "\n",
    "    # This is a low key implementation, so not for performance but for readability\n",
    "    # ('only' 2 x slower than optimized cython implementation though for the forward pass)\n",
    "    # For faster implementation, you might want to take a look at im2col and col2im methods, and Cython implementations\n",
    "    #\n",
    "    # cs231n:\n",
    "    # The input consists of N data points, each with C channels, height H and width\n",
    "    # W. We convolve each input with F different filters, where each filter spans\n",
    "    # all C channels and has height HH and width WW (HH?).\n",
    "    #\n",
    "    # Input:\n",
    "    # - x: Input data of shape (N, C, H, W)\n",
    "    # - w: Filter weights of shape (F, C, wH, wW)\n",
    "    # - b: Biases, of shape (F,)\n",
    "    # - conv_param: A dictionary with the following keys:\n",
    "    #   - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "    #     horizontal and vertical directions.\n",
    "    #   - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "    #\n",
    "    # Returns a tuple of:\n",
    "    # - out: Output data, of shape (N, F, oH, oW) where H' and W' are given by\n",
    "    #   oH = 1 + (H + 2 * pad - wH) / stride\n",
    "    #   oW = 1 + (W + 2 * pad - wW) / stride\n",
    "    # - cache: (x, w, b, conv_param)\n",
    "\n",
    "    out = None\n",
    "\n",
    "    # key values\n",
    "    N, C, h, w = inp.shape\n",
    "    F, C, Wh, Ww = W.shape\n",
    "\n",
    "    pad = divmod(Ww, 2)[0]\n",
    "    str = 1\n",
    "\n",
    "    ph = h + 2 * pad\n",
    "    pw = w + 2 * pad\n",
    "\n",
    "    oh = int(round(1 + (ph - Wh) / str))\n",
    "    ow = int(round(1 + (pw - Ww) / str))\n",
    "\n",
    "    # padding    x_p\n",
    "    x_p = np.pad(inp, ((0, 0), (0, 0), (pad, pad), (pad, pad)), 'constant')\n",
    "\n",
    "    # input column    x_col (N, C, H, W)   -> (N, C*wH*wW, oH, oW)\n",
    "    # retrieving correct indexes\n",
    "    or2d_ind = (np.arange(Wh) * ph)[:, None] + np.arange(Ww)\n",
    "    or3d_ind = (np.arange(C) * ph * pw)[:, None] + or2d_ind.ravel()\n",
    "    strides = (np.arange(oh) * ph * str)[:, None] + np.arange(ow) * str\n",
    "    str_ind = np.ravel(or3d_ind)[:, None] + strides.ravel()\n",
    "    sam_ind = ((str_ind)[None, :] + (np.arange(N) * ph * pw * C)[:, None, None])\n",
    "\n",
    "    x_col = np.take(x_p, sam_ind)\n",
    "\n",
    "    # weight column    w_col (F, C, wH, wW) -> (F, C*wH*wW)\n",
    "    w_col = np.reshape(W, (F, C * Wh * Ww))\n",
    "\n",
    "    # output column    out   (N, F, oH, oW)\n",
    "    out_col = np.einsum('ijk,lj->ilk', x_col, w_col) + b[None, :, None]\n",
    "    out = np.reshape(out_col, (N, F, oh, ow))\n",
    "\n",
    "    cache = (inp, x_col, W, b)\n",
    "    return out, cache\n",
    "\n",
    "def conv_layer_backward(dout, cache):\n",
    "    # This is a low key implementation (using for loops), so not for performance but for readability\n",
    "    # For faster implementation, you might want to take a look at im2col and col2im methods, and Cython implementations\n",
    "    #\n",
    "    # cs231n:\n",
    "    #\n",
    "    # Inputs:\n",
    "    # - dout: Upstream derivatives.\n",
    "    # - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "    #\n",
    "    # Returns a tuple of:\n",
    "    # - dx: Gradient with respect to x\n",
    "    # - dw: Gradient with respect to w\n",
    "    # - db: Gradient with respect to b\n",
    "\n",
    "    dx, dw, db = None, None, None\n",
    "\n",
    "    # forward pass cache\n",
    "    x, x_col, w, b = cache\n",
    "\n",
    "    # key values\n",
    "    N, C, H, W = x.shape\n",
    "    F, C, wH, wW = w.shape\n",
    "\n",
    "    pad = divmod(w.shape[-1],2)[0]\n",
    "    str = 1\n",
    "\n",
    "    pH = H + 2 * pad\n",
    "    pW = W + 2 * pad\n",
    "\n",
    "    oH = int(round(1 + (pH - wH) / str))\n",
    "    oW = int(round(1 + (pW - wW) / str))\n",
    "\n",
    "    # x gradients - for loop way\n",
    "    d_col = np.reshape(dout, (N, F, oH * oW))\n",
    "    w_col = np.reshape(w, (F, C * wH * wW))\n",
    "    dx_col = np.sum(np.multiply(w_col[None, :, :, None], d_col[:, :, None, :]), 1)\n",
    "    dx_cube = np.reshape(dx_col, (N, C, wH, wW, oH * oW))\n",
    "\n",
    "    dx_p = np.zeros((N, C, H + 2 * pad, W + 2 * pad), dtype=dx_col.dtype)\n",
    "    for xx in range(oW):\n",
    "        for yy in range(oH):\n",
    "            dx_p[:, :, yy * str:yy * str + wH, xx * str:xx * str + wW] += dx_cube[:, :, :, :, yy * oW + xx]\n",
    "\n",
    "    dinp = dx_p[:, :, pad:-pad, pad:-pad]\n",
    "\n",
    "    # x gradients - col2im way - about the same speed\n",
    "    # ---------------------------------------------------------------------------#\n",
    "    # dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(F, -1)\n",
    "    # dx_cols = w.reshape(F, -1).T.dot(dout_reshaped)\n",
    "    # dx = col2im_indices(dx_cols, x.shape, wH, wW, pad, str)\n",
    "\n",
    "    # x gradients - indexing way - slower\n",
    "    # ---------------------------------------------------------------------------#\n",
    "    # retrieving correct indexes\n",
    "    # C_ind    = np.tile(np.tile(np.arange(C) [:,None], wH*wW).ravel()[:, None], oH*oW)\n",
    "    #\n",
    "    # H_single = np.tile(np.tile(np.arange(wH)[:,None], wW).ravel(), C)\n",
    "    # H_stride = np.tile((np.arange(oH) * str)[:,None], oW ).ravel()\n",
    "    # H_ind    = H_single[:, None] + H_stride\n",
    "    #\n",
    "    # W_single = np.tile(np.tile(np.arange(wW),wH).ravel(), C)\n",
    "    # W_stride = np.tile((np.arange(oW) * str), oH ).ravel()\n",
    "    # W_ind    = W_single[:, None] + W_stride\n",
    "    #\n",
    "    # d_col     = np.reshape(dout, (N, F, oH*oW))\n",
    "    # w_col     = np.reshape(w, (F, C*wH*wW))\n",
    "    # dx_col    = np.sum( np.multiply(w_col[None, :, :, None], d_col[:, :, None, :]), 1)\n",
    "    #\n",
    "    # dx_p      = np.zeros((N, C, H + 2*pad, W + 2*pad), dtype=dx_col.dtype)\n",
    "    # np.add.at(dx_p, (slice(None), C_ind, H_ind, W_ind), dx_col)\n",
    "    # dx = dx_p[:, :, pad:-pad, pad:-pad]\n",
    "    # ---------------------------------------------------------------------------#\n",
    "\n",
    "    # weight gradients\n",
    "    dw_col = np.multiply(x_col[:, None, :], d_col[:, :, None, :])\n",
    "    dw_sum = np.sum(dw_col, (0, 3))\n",
    "\n",
    "    dW = np.reshape(dw_sum, (F, C, wH, wW))\n",
    "\n",
    "    # bias gradients\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "\n",
    "    return dinp, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "## MAKE SURE YOU DOWNLOAD THE MNIST DATASET YOURSELF, and put the files in the correct folder.\n",
    "## They are NOT automatically included in this repository.\n",
    "## You can easily download them [here](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images have shape:  (50000, 28, 28) training labels have shape:  (50000,)\nValidation images have shape:  (10000, 28, 28) validation labels have shape:  (10000,)\nTesting images have shape:  (10000, 28, 28) testing labels have shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# this loading code is based on Akesling's GitHub, who in turn bases his code on\n",
    "# http://abel.ee.ucla.edu/cvxopt/_downloads/mnist.py\n",
    "\n",
    "# paths to the data files. Again, make sure you put these there yourself!\n",
    "tr_img_path = 'Data_MNIST/train-images.idx3-ubyte'  # data file for training images\n",
    "tr_lbl_path = 'Data_MNIST/train-labels.idx1-ubyte'  # data file for training labels\n",
    "te_img_path = 'Data_MNIST/t10k-images.idx3-ubyte'   # data file for testing images\n",
    "te_lbl_path = 'Data_MNIST/t10k-labels.idx1-ubyte'   # data file for testing labels\n",
    "\n",
    "# import trinaing images (tr_img):\n",
    "# N_Tr is number of training samples, H is sample height (28) and W is sample width (28)\n",
    "with open(tr_img_path, 'rb') as tr_full_img_file:\n",
    "    magic, N_tr_full, H, W  = struct.unpack('>IIII', tr_full_img_file.read(16))\n",
    "    tr_full_img             = np.fromfile(tr_full_img_file, dtype=np.uint8).reshape(N_tr_full, H, W)\n",
    "\n",
    "# import training labels (tr_lbl):\n",
    "with open(tr_lbl_path, 'rb') as tr_full_lbl_file:\n",
    "    magic, _                = struct.unpack('>II', tr_full_lbl_file.read(8))\n",
    "    tr_full_lbl_1D          = np.fromfile(tr_full_lbl_file, dtype=np.uint8)\n",
    "\n",
    "# import trinaing images (tr_img):\n",
    "# N_Te is number of testing samples,\n",
    "with open(te_img_path, 'rb') as te_img_file:\n",
    "    magic, N_te, _, _       = struct.unpack('>IIII', te_img_file.read(16))\n",
    "    te_img                  = np.fromfile(te_img_file, dtype=np.uint8).reshape(N_te, H, W)\n",
    "\n",
    "# import training labels (tr_lbl):\n",
    "with open(te_lbl_path, 'rb') as te_lbl_file:\n",
    "    magic, _                = struct.unpack('>II', te_lbl_file.read(8))\n",
    "    te_lbl_1D               = np.fromfile(te_lbl_file, dtype=np.uint8)\n",
    "\n",
    "# the MNIST data set doesnt contain any validation set, so lets subsample one randomly from the training set:\n",
    "N_va   = N_te                       # decide validation set size, now taken similar to the test set size\n",
    "N_tr   = N_tr_full - N_va           # new training set size\n",
    "idx    = np.arange(N_tr+N_va)       # create N indices of the size of the old size training set\n",
    "np.random.shuffle(idx)              # random shuffle the indices\n",
    "idx_tr = idx[:N_tr ]                # get random indices for training samples\n",
    "idx_va = idx[ N_tr:]                # get rest of the random indices for validation samples\n",
    "\n",
    "tr_img = tr_full_img[idx_tr]\n",
    "va_img = tr_full_img[idx_va]\n",
    "tr_lbl_1D = tr_full_lbl_1D[idx_tr]\n",
    "va_lbl_1D = tr_full_lbl_1D[idx_va]\n",
    "\n",
    "print('Training images have shape: ',tr_img.shape, 'training labels have shape: ',tr_lbl_1D.shape)\n",
    "print('Validation images have shape: ',va_img.shape, 'validation labels have shape: ',va_lbl_1D.shape)\n",
    "print('Testing images have shape: ',te_img.shape, 'testing labels have shape: ',te_lbl_1D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Currently, the labels are 1D numpy arrays with entries in [0-9]. To classify the images with a Neural Network, it is easier to make the labels one-hot encoded 2D numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old training labels have shape:  (50000,) new training labels have shape:  (50000, 10)\nold validation labels have shape:  (10000,) new validation labels have shape:  (10000, 10)\nold testing labels have shape:  (10000,) new testing labels have shape:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# one hot encode the training labels\n",
    "C      = np.max(tr_lbl_1D) + 1                      # number of classes (+1 cause 0 is also a class)\n",
    "tr_lbl = np.zeros((N_tr, C))                        # initialise empty an empty array for the one hot encoded training labels\n",
    "tr_lbl[np.arange(N_tr), tr_lbl_1D.astype(int)] = 1  # one hot encode the training labels\n",
    "\n",
    "# same, for the validation labels\n",
    "va_lbl = np.zeros((N_va, C))\n",
    "va_lbl[np.arange(N_va), va_lbl_1D.astype(int)] = 1\n",
    "\n",
    "# and the same for the testing labels\n",
    "te_lbl = np.zeros((N_te, C))\n",
    "te_lbl[np.arange(N_te), te_lbl_1D.astype(int)] = 1\n",
    "\n",
    "print('old training labels have shape: ',tr_lbl_1D.shape, 'new training labels have shape: ', tr_lbl.shape)\n",
    "print('old validation labels have shape: ',va_lbl_1D.shape,  'new validation labels have shape: ',  va_lbl.shape)\n",
    "print('old testing labels have shape: ',te_lbl_1D.shape,  'new testing labels have shape: ',  te_lbl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show data\n",
    "### Alway good to know the data we're working with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H & W of the images: 28 ,  28\n# of classes: 10\nrandom example:\nimage class/label one hot: [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.] , or non one hot: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1N/NDiNnodQgAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x180b11a46a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get random sample index, rerun this cell for different samples\n",
    "sample_id = np.random.randint(tr_img.shape[0])\n",
    "\n",
    "print('H & W of the images:', H, ', ', W)\n",
    "print('# of classes:', C)\n",
    "print('random example:')\n",
    "print('image class/label one hot:', tr_lbl[sample_id], ', or non one hot:', np.argmax(tr_lbl[sample_id]))\n",
    "\n",
    "plt.imshow(tr_img[sample_id], cmap=plt.gray())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to debug, or just check inside the code, it might we worth it to take a smaller dataset for speed up:\n",
    "# remember to reload the entire data and make it one-hot again if you want to run on the entire data set\n",
    "tr_img = tr_img[:1000]\n",
    "tr_lbl = tr_lbl[:1000]\n",
    "va_img = va_img[:1000]\n",
    "va_lbl = va_lbl[:1000]\n",
    "te_img = te_img[:1000]\n",
    "te_lbl = te_lbl[:1000]\n",
    "N_tr, N_va, N_te = 1000, 1000, 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the model class:\n",
    "### This imports a trainable network, please look inside for the API.\n",
    "### It implements a simple, fully connected net with a flexible amount and size of the hidden layers. Furthermore, it includes dropout and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    \"\"\"\n",
    "    Implements a simple Neural Network not unlike a vanilla net\n",
    "    The only differences are an implementation of dropout and Batch Normalisation in every forward and backward layer\n",
    "\n",
    "    arguments:\n",
    "    input_size       = number of features per sample in the training data\n",
    "    layer_sizes      = list or numpy array, with each entry i corresponding to the amount of hidden neurons in the ith hidden layer\n",
    "    output_size      = number of output classes for prediction\n",
    "\n",
    "    functions:\n",
    "    forward_pass()\n",
    "    backward_pass()\n",
    "    test()\n",
    "    train()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, layer_sizes, output_size):\n",
    "\n",
    "\n",
    "        # store the __init__ variables for other functions\n",
    "        self.D        = input_size\n",
    "        self.lay_size = layer_sizes\n",
    "        self.C        = output_size\n",
    "\n",
    "        # create layer variable weights and biases in a dictionary\n",
    "        self.var = {}\n",
    "        prev_size = np.hstack((self.D, layer_sizes))                              # temporary lookup array for weight variable sizes\n",
    "\n",
    "        for i, size in enumerate(layer_sizes):                                    # iterate over each layer\n",
    "\n",
    "            cur_W             = ('W%d' % (i+1))\n",
    "            self.var[cur_W]   = np.random.normal(0.0, 0.1, (prev_size[i], size))  # add weight variable\n",
    "            cur_b             = ('b%d' % (i+1))\n",
    "            self.var[cur_b]   = np.zeros(size)                                    # add bias variable\n",
    "            cur_gam           = ('gam%d' % (i+1))\n",
    "            self.var[cur_gam] = np.zeros(size)                                    # add gamma variable\n",
    "            cur_bet           = ('bet%d' % (i+1))\n",
    "            self.var[cur_bet] = np.zeros(size)                                    # add beta variable\n",
    "\n",
    "        self.var['Wout'] = np.random.normal(0.0, 0.1, (layer_sizes[-1], self.C))  # add output layer weight variable\n",
    "        self.var['bout'] = np.zeros(self.C)                                       # add output layer bias variable\n",
    "\n",
    "        # keep track of bn_params\n",
    "        self.bn_params = []\n",
    "        self.bn_params = [{'mode': 'train'} for i in range(len(layer_sizes))]\n",
    "\n",
    "    def forward_pass(self, X, y, drop_prob):\n",
    "        \"\"\"\n",
    "        A single forward pass, affine layers using the sigmoid nonlinearity activation function\n",
    "        Performs dropout and Batch Normalization after affine and before activation\n",
    "        Last layer is implements an affine layer plus a softmax\n",
    "        Also calculates the loss, prediction, and prediction derivative\n",
    "\n",
    "        arguments:\n",
    "        X       = numpy matrix, minibatch of N samples, each with self.D features\n",
    "        y       = numpy matrix, minibatch of one hot encoded labels for same N samples as X\n",
    "\n",
    "        returns:\n",
    "        loss    = scalar,       softmax prediction loss for current X and y, does NOT contain regularisation loss\n",
    "        pred    = numpy array,  predicted class for each of N samples\n",
    "        acc     = scalar,       percentage of correctly predicted classes for current X and y\n",
    "        dpred   = numpy_array,  derivative of the prediction, (including backprop through softmax layer), internal use for backprop\n",
    "        cache   = dictionary,   keeps track of all the cahce necessary for backpropagation later on\n",
    "        \"\"\"\n",
    "\n",
    "        # create cache dictionary to keep around data required for backward pass\n",
    "        cache = {}\n",
    "\n",
    "        # hidden layer calculations\n",
    "        inp = X                                                                    # initialise input for following layer iteration\n",
    "        for i, size in enumerate(self.lay_size):                                   # iterate over each layer\n",
    "\n",
    "            cur_W                  = self.var[('W%d' % (i+1))]\n",
    "            cur_b                  = self.var[('b%d' % (i+1))]\n",
    "            cur_gam                = self.var[('gam%d' % (i+1))]\n",
    "            cur_bet                = self.var[('bet%d' % (i+1))]\n",
    "            h, cur_cache           = BN_Dr_sig_layer_forward(inp, cur_W, cur_b,    # forward layer calculation\n",
    "                                                             drop_prob,            # dropout parameters\n",
    "                                                             cur_gam, cur_bet, self.bn_params[i])   # batch normalization parameters\n",
    "            inp                    = h\n",
    "            cache[('c%d' % (i+1))] = cur_cache\n",
    "\n",
    "\n",
    "        # output layer calculation\n",
    "        pred_sof, cout = softmax_layer_forward(h, self.var['Wout'], self.var['bout']) # final forward layer calculation\n",
    "        cache['cout']  = cout\n",
    "        pred           = np.argmax(pred_sof, axis=1)                               # prediction from softmax distribution to specific class\n",
    "        acc            = np.sum(pred == np.argmax(y, axis=1)) * 1.0 / X.shape[0]   # final forward layer calculation\n",
    "\n",
    "\n",
    "        # loss & prediction derivative calculation\n",
    "        loss, dpred                = cross_entropy_loss(pred_sof, y)\n",
    "\n",
    "\n",
    "        return loss, pred, acc, dpred, cache\n",
    "\n",
    "\n",
    "    def backward_pass(self, dpred, cache):\n",
    "        \"\"\"\n",
    "        implements backpropagation through all the layers in the network, saves the derivatives (does NOT update variables!)\n",
    "\n",
    "        arguments:\n",
    "        dpred   = numpy matrix, contains the derivatives for the output of the final layer\n",
    "        cache   = dictionary,   forward pass cache necessary for backward pass derivative calculations\n",
    "\n",
    "        returns:\n",
    "        none\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate layer weights and bias derivatives in a dictionary\n",
    "        self.der = {}\n",
    "\n",
    "        # output layer calculation\n",
    "        dh, dW, db       = affine_layer_backward(dpred, cache['cout'])             # backpropagate through final layer without softmax\n",
    "        dout             = dh                                                      # initialise output deriv for following layer iteration\n",
    "        self.der['Wout'] = dW                                                      # update output layer weight derivatives\n",
    "        self.der['bout'] = db                                                      # update output layer bias derivatives\n",
    "\n",
    "        # hidden layer calculations\n",
    "        for i_unrev, size in enumerate(reversed(self.lay_size)):                   # iterate backwards over each layer\n",
    "\n",
    "            i                         = len(self.lay_size)-i_unrev                 # create reversed i. +1 not necessary due to len()\n",
    "            cur_cac                   = cache[('c%d' % (i))]\n",
    "            dh, dW, db, dgam, dbet    = BN_Dr_sig_layer_backward(dout, cur_cac)    # backpropagate through layer\n",
    "            dout                      = dh\n",
    "            self.der[('W%d' % (i))]   = dW                                         # update weight derivatives for layer i\n",
    "            self.der[('b%d' % (i))]   = db                                         # update bias derivatives layer i\n",
    "            self.der[('gam%d' % (i))] = dgam                                       # update gamma (BN) derivatives layer i\n",
    "            self.der[('bet%d' % (i))] = dbet                                       # update beta (BN) derivatives layer i\n",
    "\n",
    "\n",
    "    def test(self, X, y):\n",
    "        \"\"\"\n",
    "        implements a forward pass and returns the prediction for testing\n",
    "\n",
    "        arguments:\n",
    "        X, y    = like in forward_pass()\n",
    "\n",
    "        returns:\n",
    "        loss    = like in forward_pass()\n",
    "        pred    =   ,,\n",
    "        acc     =   ,,\n",
    "        \"\"\"\n",
    "\n",
    "        loss, pred, acc, _, _ = self.forward_pass(X, y, drop_prob=0.0)\n",
    "\n",
    "        return loss, pred, acc\n",
    "\n",
    "\n",
    "    def train(self, X_tr, y_tr, X_va, y_va, number_epochs, batch_size, learning_rate, reg_strength, drop_prob, print_every, verbose):\n",
    "        \"\"\"\n",
    "        this is the core function, it actually trains our model. It keeps track of the best performance on the validation set.\n",
    "        Final variables are taken from this best performance\n",
    "\n",
    "        arguments:\n",
    "        X_tr, X_va = numpy matrix, minibatch of N samples, each with self.D features\n",
    "        y_tr, y_va =\n",
    "        number_epochs = int,    total number of epochs for training\n",
    "        batch_size    = int,    batch size of training samples per forward and backward pass\n",
    "        learning_rate = scalar, learning rate for stochastic gradient descent\n",
    "        reg_strength  = scalar, regularization strenght (L2 implemented)\n",
    "        print_every   = int,    print performance during training every this amount of iterations\n",
    "        verbose       = bool,   print performance during training yes or no\n",
    "\n",
    "        returns:\n",
    "        tr_loss_hist  = list,   containing training set loss history (without regularisation loss!)\n",
    "        va_loss_hist  = list,   containing validation set loss history (without regularisation loss!)\n",
    "        tr_acc_hist   = list,   containing training set accuracy history\n",
    "        va_acc_hist   = list,   containing validation set accuracy history\n",
    "        \"\"\"\n",
    "\n",
    "        # keep the best performing variables around during training:\n",
    "        best_va_var  = {}\n",
    "        best_va_acc  = 0\n",
    "\n",
    "        # initialise lists to keep track of the training history\n",
    "        tr_acc_hist  = []\n",
    "        va_acc_hist  = []\n",
    "        tr_loss_hist = []\n",
    "        va_loss_hist = []\n",
    "\n",
    "        # some dependent parameters\n",
    "        N              = X_tr.shape[0]\n",
    "        iterations     = N * number_epochs / batch_size\n",
    "        iter_per_epoch = iterations / number_epochs\n",
    "        #X_ba, y_ba = get_random_batch(X_tr, y_tr, batch_size) # uncomment this line for the 'overfit hack'\n",
    "        for e in range(number_epochs):          # iterate over epochs\n",
    "\n",
    "            for i in range(int(iter_per_epoch)):     # iterate over iterations per epochs\n",
    "\n",
    "                # get current validation performance\n",
    "                va_loss, _, va_acc  = self.test(X_va, y_va)\n",
    "\n",
    "                # forward and backpropagate\n",
    "                X_ba, y_ba = get_random_batch(X_tr, y_tr, batch_size) # comment this line out for the 'overfit hack'\n",
    "                tr_loss, pred, tr_acc, dpred, cache = self.forward_pass(X_ba, y_ba, drop_prob)\n",
    "                self.backward_pass(dpred, cache)\n",
    "\n",
    "                # update loss histories\n",
    "                tr_acc_hist.append(tr_acc)\n",
    "                va_acc_hist.append(va_acc)\n",
    "                tr_loss_hist.append(tr_loss)\n",
    "                va_loss_hist.append(va_loss)\n",
    "\n",
    "                # update the best validation set performing variables\n",
    "                if va_acc > best_va_acc:\n",
    "                    best_va_acc = copy.deepcopy(va_acc)\n",
    "                    best_va_var = copy.deepcopy(self.var)\n",
    "\n",
    "                # update variables\n",
    "                for var_key, var  in self.var.items():\n",
    "                    if 'W' in var_key:                                     # only regularise W matrices, not the bias arrays\n",
    "                        self.var[var_key] -= reg_strength  * var           # regularisation\n",
    "                    self.var[var_key] -= learning_rate * self.der[var_key] # derivatives\n",
    "\n",
    "                # print output during training\n",
    "                if (e*iter_per_epoch + i) % print_every == 0 and verbose:\n",
    "                    print('iteration %4d/%4d, training accuracy: %.2f, training loss: %.4f' \\\n",
    "                          % (e*iter_per_epoch + i, iterations, tr_acc, tr_loss))\n",
    "\n",
    "            # print output during training per epoch\n",
    "            #if verbose:\n",
    "            #    print('epoch %2d/%2d, validation accuracy: %.2f'% (e+1, number_epochs, va_acc))\n",
    "\n",
    "        # append final accuracies to history\n",
    "        _, _, tr_acc = self.test(X_tr, y_tr)\n",
    "        _, _, va_acc = self.test(X_va, y_va)\n",
    "        tr_acc_hist.append(tr_acc)\n",
    "        va_acc_hist.append(va_acc)\n",
    "\n",
    "        # final check for the best validation set performing variables\n",
    "        if va_acc > best_va_acc:\n",
    "            best_va_acc = copy.deepcopy(va_acc)\n",
    "            best_va_var = copy.deepcopy(self.var)\n",
    "\n",
    "        # replace variables with the best performing ones\n",
    "        self.var = {}\n",
    "        self.var = best_va_var\n",
    "\n",
    "        return tr_loss_hist, va_loss_hist, tr_acc_hist, va_acc_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for the fun stuff! Let's train out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration    0/250000, training accuracy: 0.00, training loss: 2.3357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   50/250000, training accuracy: 0.00, training loss: 2.3151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  100/250000, training accuracy: 0.00, training loss: 2.2948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  150/250000, training accuracy: 0.00, training loss: 2.2748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  200/250000, training accuracy: 0.00, training loss: 2.2552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  250/250000, training accuracy: 0.00, training loss: 2.2358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  300/250000, training accuracy: 0.00, training loss: 2.2167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  350/250000, training accuracy: 0.00, training loss: 2.1979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  400/250000, training accuracy: 0.00, training loss: 2.1794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  450/250000, training accuracy: 0.00, training loss: 2.1612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  500/250000, training accuracy: 0.00, training loss: 2.1433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  550/250000, training accuracy: 0.50, training loss: 2.1257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  600/250000, training accuracy: 0.50, training loss: 2.1085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  650/250000, training accuracy: 0.50, training loss: 2.0915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  700/250000, training accuracy: 0.50, training loss: 2.0748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  750/250000, training accuracy: 0.50, training loss: 2.0583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  800/250000, training accuracy: 0.50, training loss: 2.0422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  850/250000, training accuracy: 0.50, training loss: 2.0264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  900/250000, training accuracy: 0.50, training loss: 2.0108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  950/250000, training accuracy: 0.50, training loss: 1.9956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000/250000, training accuracy: 0.50, training loss: 1.9806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1050/250000, training accuracy: 0.50, training loss: 1.9659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100/250000, training accuracy: 0.50, training loss: 1.9515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1150/250000, training accuracy: 0.50, training loss: 1.9373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200/250000, training accuracy: 0.50, training loss: 1.9235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1250/250000, training accuracy: 0.50, training loss: 1.9099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300/250000, training accuracy: 0.50, training loss: 1.8965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1350/250000, training accuracy: 0.50, training loss: 1.8834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400/250000, training accuracy: 0.50, training loss: 1.8706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1450/250000, training accuracy: 0.50, training loss: 1.8580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1500/250000, training accuracy: 0.50, training loss: 1.8457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1550/250000, training accuracy: 0.50, training loss: 1.8337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1600/250000, training accuracy: 0.50, training loss: 1.8218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1650/250000, training accuracy: 0.50, training loss: 1.8102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1700/250000, training accuracy: 0.50, training loss: 1.7989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1750/250000, training accuracy: 0.50, training loss: 1.7878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1800/250000, training accuracy: 0.50, training loss: 1.7769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1850/250000, training accuracy: 0.50, training loss: 1.7663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1900/250000, training accuracy: 0.50, training loss: 1.7558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1950/250000, training accuracy: 0.50, training loss: 1.7456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2000/250000, training accuracy: 0.50, training loss: 1.7356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2050/250000, training accuracy: 0.50, training loss: 1.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2100/250000, training accuracy: 0.50, training loss: 1.7162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2150/250000, training accuracy: 0.50, training loss: 1.7068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2200/250000, training accuracy: 0.50, training loss: 1.6976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2250/250000, training accuracy: 0.50, training loss: 1.6887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2300/250000, training accuracy: 0.50, training loss: 1.6798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2350/250000, training accuracy: 0.50, training loss: 1.6712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2400/250000, training accuracy: 0.50, training loss: 1.6628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2450/250000, training accuracy: 0.50, training loss: 1.6545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2500/250000, training accuracy: 0.50, training loss: 1.6464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2550/250000, training accuracy: 0.50, training loss: 1.6385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2600/250000, training accuracy: 0.50, training loss: 1.6308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2650/250000, training accuracy: 0.50, training loss: 1.6232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2700/250000, training accuracy: 0.50, training loss: 1.6158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2750/250000, training accuracy: 0.50, training loss: 1.6085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2800/250000, training accuracy: 0.50, training loss: 1.6014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2850/250000, training accuracy: 0.50, training loss: 1.5944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2900/250000, training accuracy: 0.50, training loss: 1.5876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2950/250000, training accuracy: 0.50, training loss: 1.5809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3000/250000, training accuracy: 0.50, training loss: 1.5743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3050/250000, training accuracy: 0.50, training loss: 1.5679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3100/250000, training accuracy: 0.50, training loss: 1.5617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3150/250000, training accuracy: 0.50, training loss: 1.5555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3200/250000, training accuracy: 0.50, training loss: 1.5495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3250/250000, training accuracy: 0.50, training loss: 1.5436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3300/250000, training accuracy: 0.50, training loss: 1.5378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3350/250000, training accuracy: 0.50, training loss: 1.5321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3400/250000, training accuracy: 0.50, training loss: 1.5265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3450/250000, training accuracy: 0.50, training loss: 1.5211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3500/250000, training accuracy: 0.50, training loss: 1.5157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3550/250000, training accuracy: 0.50, training loss: 1.5105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3600/250000, training accuracy: 0.50, training loss: 1.5053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3650/250000, training accuracy: 0.50, training loss: 1.5003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3700/250000, training accuracy: 0.50, training loss: 1.4953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3750/250000, training accuracy: 0.50, training loss: 1.4905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3800/250000, training accuracy: 0.50, training loss: 1.4857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6c70f8257681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                                                                     \u001b[0mdrop_prob\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                                                     \u001b[0mprint_every\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                                                                     verbose       = True)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nfinal training loss: %.4f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtr_loss_hist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-39c1f9682741>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_tr, y_tr, X_va, y_va, number_epochs, batch_size, learning_rate, reg_strength, drop_prob, print_every, verbose)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[1;31m# get current validation performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mva_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mva_acc\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_va\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_va\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[1;31m# forward and backpropagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-39c1f9682741>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[1;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-39c1f9682741>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, X, y, drop_prob)\u001b[0m\n\u001b[1;32m     78\u001b[0m             h, cur_cache           = BN_Dr_sig_layer_forward(inp, cur_W, cur_b,    # forward layer calculation\n\u001b[1;32m     79\u001b[0m                                                              \u001b[0mdrop_prob\u001b[0m\u001b[1;33m,\u001b[0m            \u001b[1;31m# dropout parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                                                              cur_gam, cur_bet, self.bn_params[i])   # batch normalization parameters\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0minp\u001b[0m                    \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'c%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-c87dcc34cd53>\u001b[0m in \u001b[0;36mBN_Dr_sig_layer_forward\u001b[0;34m(inp, W, b, drop, gam, bet, bn_param)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mBN_Dr_sig_layer_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[1;31m# implements vanilla foward neural net layer, then BN, then Dropout, then a sigmoid activation, assumes all input to be numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mun_act\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc_cache\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0maffine_layer_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mBN\u001b[0m\u001b[1;33m,\u001b[0m     \u001b[0mBN_cache\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mbatch_norm_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mun_act\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mDrop\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[0mDr_cache\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mdropout_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-c87dcc34cd53>\u001b[0m in \u001b[0;36maffine_layer_forward\u001b[0;34m(inp, W, b)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maffine_layer_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[1;31m# implements vanilla foward neural net layer, assumes all input to be numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mout\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Hyperparameters, you should play around with these!:\n",
    "\n",
    "layer_sizes = [100, 50, 25]  # size per hidden layer\n",
    "\n",
    "num_epoch   = 1000            # total number of epochs for training\n",
    "batch_size  = 4             # batch size of training samples per forward and backward pass\n",
    "lrn_rate    = 5e-5            # learning rate for stochastic gradient descent\n",
    "reg_str     = 1e-6            # regularization strenght (L2 implemented)\n",
    "drop_prob   = 0.5             # the probability with which every neuron is dropped, should be in range [0-1) (NOT 1!)\n",
    "print_every = 50              # during training, print current performance every this amount of iterations\n",
    "\n",
    "# a Vanilla net takes a flat feature array as input instead of a 2D image, so change the shape of the images:\n",
    "tr_img_flat = tr_img.reshape(N_tr, -1)\n",
    "va_img_flat = va_img.reshape(N_va, -1)\n",
    "te_img_flat = te_img.reshape(N_te, -1)\n",
    "\n",
    "# initialise a Vanilla network class, please look in the Models.py file for its API and workings\n",
    "Net = NeuralNet(input_size       = (H*W),\n",
    "                layer_sizes      = layer_sizes,\n",
    "                output_size      = C)\n",
    "# train\n",
    "tr_loss_hist, va_loss_hist, tr_acc_hist, va_acc_hist = Net.train(tr_img_flat, tr_lbl, va_img_flat, va_lbl,\n",
    "                                                                    number_epochs = num_epoch,\n",
    "                                                                    batch_size    = batch_size,\n",
    "                                                                    learning_rate = lrn_rate,\n",
    "                                                                    reg_strength  = reg_str,\n",
    "                                                                    drop_prob     = drop_prob,\n",
    "                                                                    print_every   = print_every,\n",
    "                                                                    verbose       = True)\n",
    "\n",
    "print('\\nfinal training loss: %.4f' % (tr_loss_hist[-1]))\n",
    "print('final training accuracy: %.2f, best validation accuracy: %.2f' % (tr_acc_hist[-1], np.max(va_acc_hist)))\n",
    "\n",
    "# test\n",
    "loss, pred, final_test_accuracy = Net.test(te_img_flat, te_lbl)\n",
    "print('\\nfinal test accuracy: %.2f' % final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise the training of your last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot inside the ipython notebook\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "\n",
    "# plot training set and validation set loss histories\n",
    "plt.subplot(2, 1, 1)\n",
    "ep_axis = np.arange(len(tr_loss_hist)) * 1.0 / len(tr_loss_hist) * num_epoch\n",
    "plt.title('Training loss (softmax loss without regularisatioin loss)[-]')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(ep_axis, tr_loss_hist, label='training set')\n",
    "plt.plot(ep_axis, va_loss_hist, label='validation set')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# plot training set and validation set accuracies\n",
    "plt.subplot(2, 1, 2)\n",
    "ep_axis = np.arange(len(tr_acc_hist)) * 1.0 / len(tr_acc_hist) * num_epoch\n",
    "plt.title('Training & Validation accuracy (%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(ep_axis, tr_acc_hist, label='train')\n",
    "plt.plot(ep_axis, va_acc_hist, label='validation')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Discussion\n",
    "After playing around with the model a bit, a few key characteristics are interesting to notice:\n",
    "\n",
    "#### robustness and convergence\n",
    "The model is quite robust, but also trains extremely slow, somtimes even plateauing the accuracy and loss in the beginning.\n",
    "A quick hack to check the validity of the model, is to limit the batches the batch size, and feed the network the same batch every iteration: i.e. overfitting the model. (some comment talk about this and call it the 'overfit hack')\n",
    "\n",
    "\n",
    "Once we do this, it is possible to learn to a 100%, meaning the model is indeed valid.\n",
    "\n",
    "The problems for convergence are therefore likely along the line of:\n",
    "1. The model is not powerfull enough to fit the MNIST data set (likely since we are not even using the convolutional nets)\n",
    "2. The implementation is not efficient and robust enough\n",
    "\n",
    "#### regularisation\n",
    "Dropout and batch normalisation act as regularizers, reducing the need for the L2 regularization. BN alone however is not enough, because when both dropout and L2 strenght are put to 0, the validation accuracy lacks compared to training accuracy. It seems none of the three is able to perform enough regularization on its own.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
