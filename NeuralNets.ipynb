{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "\n",
    "This ipython notebook implements three simple Neural Nets that try to classify the MNIST dataset:\n",
    "* (Vanilla_NN): Vanilla Neural Network\n",
    "* (BN_Drop_NN): Slightly fancier Neural Network with Dropout and Batch Normalization\n",
    "* (Conv_NN): Simple Convolutional Neural Network\n",
    "\n",
    "This code is aimed for those who understand the basics of Neural Networks, and want to see an actual practical implementation.\n",
    "\n",
    "As such, this code doesn't use any higher level libraries such as Tensorflow. It only makes extensive use of Numpy.\n",
    "\n",
    "Hopefully this will give you an easy to follow code and a good overview of what happens inside the Neural Nets step by step.\n",
    "\n",
    "\n",
    "Files description:\n",
    "\n",
    "* Data_MNIST:\t\tfolder that should contain all the data, please download these yourself \n",
    "* Models.py:             script that contains athe three networks as python classes\n",
    "* NN_utils.py:           script that contains all library of main neural network functions\n",
    "* NeuralNets.ipynb       (this file) ipython notebook that instantiates the network classes from Models.py, and trains them etc. More description inside.\n",
    "\n",
    "Lacking:\n",
    "\n",
    "* Actually, the three models are not that different. It would be nice to merge them, with API that lets you toggle on/off BN and dropout and conv\n",
    "* Now each net is its own class, would be easier to create new nets if they were all just daughters of a parent general net class\n",
    "* ReLU layer, other activation functions\n",
    "* Visualize different nets training characteristics next to each other\n",
    "* max Pool or other pooling function would be nice addition\n",
    "* variable stride and padding for conv nets\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import struct\n",
    "import numpy as np\n",
    "from Models   import *\n",
    "from nn_utils import *\n",
    "\n",
    "# ipython magic reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "## MAKE SURE YOU DOWNLOAD THE MNIST DATASET YOURSELF, and put the files in the correct folder.\n",
    "## They are NOT automatically included in this repository.\n",
    "## You can easily download them from: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images have shape:  (50000, 28, 28) training labels have shape:  (50000,)\nValidation images have shape:  (10000, 28, 28) validation labels have shape:  (10000,)\nTesting images have shape:  (10000, 28, 28) testing labels have shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# this loading code is based on Akesling's GitHub, who in turn bases his code on\n",
    "# http://abel.ee.ucla.edu/cvxopt/_downloads/mnist.py\n",
    "\n",
    "# paths to the data files. Again, make sure you put these there yourself!\n",
    "tr_img_path = 'Data_MNIST/train-images.idx3-ubyte'  # data file for training images\n",
    "tr_lbl_path = 'Data_MNIST/train-labels.idx1-ubyte'  # data file for training labels\n",
    "te_img_path = 'Data_MNIST/t10k-images.idx3-ubyte'   # data file for testing images\n",
    "te_lbl_path = 'Data_MNIST/t10k-labels.idx1-ubyte'   # data file for testing labels\n",
    "\n",
    "# import trinaing images (tr_img):\n",
    "# N_Tr is number of training samples, H is sample height (28) and W is sample width (28)\n",
    "with open(tr_img_path, 'rb') as tr_full_img_file:\n",
    "    magic, N_tr_full, H, W  = struct.unpack('>IIII', tr_full_img_file.read(16))\n",
    "    tr_full_img             = np.fromfile(tr_full_img_file, dtype=np.uint8).reshape(N_tr_full, H, W)\n",
    "\n",
    "# import training labels (tr_lbl):\n",
    "with open(tr_lbl_path, 'rb') as tr_full_lbl_file:\n",
    "    magic, _                = struct.unpack('>II', tr_full_lbl_file.read(8))\n",
    "    tr_full_lbl_1D          = np.fromfile(tr_full_lbl_file, dtype=np.uint8)\n",
    "\n",
    "# import trinaing images (tr_img):\n",
    "# N_Te is number of testing samples,\n",
    "with open(te_img_path, 'rb') as te_img_file:\n",
    "    magic, N_te, _, _       = struct.unpack('>IIII', te_img_file.read(16))\n",
    "    te_img                  = np.fromfile(te_img_file, dtype=np.uint8).reshape(N_te, H, W)\n",
    "\n",
    "# import training labels (tr_lbl):\n",
    "with open(te_lbl_path, 'rb') as te_lbl_file:\n",
    "    magic, _                = struct.unpack('>II', te_lbl_file.read(8))\n",
    "    te_lbl_1D               = np.fromfile(te_lbl_file, dtype=np.uint8)\n",
    "\n",
    "# the MNIST data set doesnt contain any validation set, so lets subsample one randomly from the training set:\n",
    "N_va   = N_te                       # decide validation set size, now taken similar to the test set size\n",
    "N_tr   = N_tr_full - N_va           # new training set size\n",
    "idx    = np.arange(N_tr+N_va)       # create N indices of the size of the old size training set\n",
    "np.random.shuffle(idx)              # random shuffle the indices\n",
    "idx_tr = idx[:N_tr ]                # get random indices for training samples\n",
    "idx_va = idx[ N_tr:]                # get rest of the random indices for validation samples\n",
    "\n",
    "tr_img = tr_full_img[idx_tr]\n",
    "va_img = tr_full_img[idx_va]\n",
    "tr_lbl_1D = tr_full_lbl_1D[idx_tr]\n",
    "va_lbl_1D = tr_full_lbl_1D[idx_va]\n",
    "\n",
    "print('Training images have shape: ',tr_img.shape, 'training labels have shape: ',tr_lbl_1D.shape)\n",
    "print('Validation images have shape: ',va_img.shape, 'validation labels have shape: ',va_lbl_1D.shape)\n",
    "print('Testing images have shape: ',te_img.shape, 'testing labels have shape: ',te_lbl_1D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Currently, the labels are 1D numpy arrays with entries in [0-9]. To classify the images with a Neural Network, it is easier to make the labels one-hot encoded 2D numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old training labels have shape:  (50000,) new training labels have shape:  (50000, 10)\nold validation labels have shape:  (10000,) new validation labels have shape:  (10000, 10)\nold testing labels have shape:  (10000,) new testing labels have shape:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# one hot encode the training labels\n",
    "C      = np.max(tr_lbl_1D) + 1                      # number of classes (+1 cause 0 is also a class)\n",
    "tr_lbl = np.zeros((N_tr, C))                        # initialise empty an empty array for the one hot encoded training labels\n",
    "tr_lbl[np.arange(N_tr), tr_lbl_1D.astype(int)] = 1  # one hot encode the training labels\n",
    "\n",
    "# same, for the validation labels\n",
    "va_lbl = np.zeros((N_va, C))\n",
    "va_lbl[np.arange(N_va), va_lbl_1D.astype(int)] = 1\n",
    "\n",
    "# and the same for the testing labels\n",
    "te_lbl = np.zeros((N_te, C))\n",
    "te_lbl[np.arange(N_te), te_lbl_1D.astype(int)] = 1\n",
    "\n",
    "print('old training labels have shape: ',tr_lbl_1D.shape, 'new training labels have shape: ', tr_lbl.shape)\n",
    "print('old validation labels have shape: ',va_lbl_1D.shape,  'new validation labels have shape: ',  va_lbl.shape)\n",
    "print('old testing labels have shape: ',te_lbl_1D.shape,  'new testing labels have shape: ',  te_lbl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show data\n",
    "### Alway good to know the data we're working with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H & W of the images: 28 ,  28\n# of classes: 10\nrandom example:\nimage class/label one hot: [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.] , or non one hot: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22c36288898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get random sample index, rerun this cell for different samples\n",
    "sample_id = np.random.randint(tr_img.shape[0])\n",
    "\n",
    "print('H & W of the images:', H, ', ', W)\n",
    "print('# of classes:', C)\n",
    "print('random example:')\n",
    "print('image class/label one hot:', tr_lbl[sample_id], ', or non one hot:', np.argmax(tr_lbl[sample_id]))\n",
    "\n",
    "plt.imshow(tr_img[sample_id], cmap=plt.gray())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to debug, or just check inside the code, it might we worth it to take a smaller dataset for speed up\n",
    "\n",
    "tr_img = tr_img[:1000]\n",
    "tr_lbl = tr_lbl[:1000]\n",
    "va_img = va_img[:1000]\n",
    "va_lbl = va_lbl[:1000]\n",
    "te_img = te_img[:1000]\n",
    "te_lbl = te_lbl[:1000]\n",
    "N_tr, N_va, N_te = 1000, 1000, 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for the fun stuff! First the Vanilla_NN network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anner\\Documents\\Werk\\LeapMind\\nn_utils.py:179: RuntimeWarning: overflow encountered in exp\n  out   = 1 / (1 + np.exp(-un_act))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration    0/1000, training accuracy: 0.06, training loss: 2.3568\niteration    2/1000, training accuracy: 0.16, training loss: 2.2040\niteration    4/1000, training accuracy: 0.16, training loss: 2.1947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration    6/1000, training accuracy: 0.16, training loss: 2.1918\niteration    8/1000, training accuracy: 0.16, training loss: 2.1900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   10/1000, training accuracy: 0.16, training loss: 2.1891\niteration   12/1000, training accuracy: 0.16, training loss: 2.1892\niteration   14/1000, training accuracy: 0.16, training loss: 2.1884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   16/1000, training accuracy: 0.16, training loss: 2.1876\niteration   18/1000, training accuracy: 0.16, training loss: 2.1871\niteration   20/1000, training accuracy: 0.16, training loss: 2.1860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   22/1000, training accuracy: 0.16, training loss: 2.1844\niteration   24/1000, training accuracy: 0.16, training loss: 2.1836\niteration   26/1000, training accuracy: 0.16, training loss: 2.1821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   28/1000, training accuracy: 0.16, training loss: 2.1818\niteration   30/1000, training accuracy: 0.16, training loss: 2.1807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   32/1000, training accuracy: 0.17, training loss: 2.1796\niteration   34/1000, training accuracy: 0.19, training loss: 2.1789\niteration   36/1000, training accuracy: 0.19, training loss: 2.1770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   38/1000, training accuracy: 0.20, training loss: 2.1758\niteration   40/1000, training accuracy: 0.20, training loss: 2.1746\niteration   42/1000, training accuracy: 0.20, training loss: 2.1713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   44/1000, training accuracy: 0.21, training loss: 2.1689\niteration   46/1000, training accuracy: 0.21, training loss: 2.1674\niteration   48/1000, training accuracy: 0.21, training loss: 2.1651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   50/1000, training accuracy: 0.20, training loss: 2.1650\niteration   52/1000, training accuracy: 0.20, training loss: 2.1618\niteration   54/1000, training accuracy: 0.19, training loss: 2.1588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   56/1000, training accuracy: 0.18, training loss: 2.1539\niteration   58/1000, training accuracy: 0.16, training loss: 2.1487\niteration   60/1000, training accuracy: 0.16, training loss: 2.1626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   62/1000, training accuracy: 0.16, training loss: 2.1841\niteration   64/1000, training accuracy: 0.22, training loss: 2.0779\niteration   66/1000, training accuracy: 0.16, training loss: 2.1801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   68/1000, training accuracy: 0.20, training loss: 2.1739\niteration   70/1000, training accuracy: 0.31, training loss: 1.9944\niteration   72/1000, training accuracy: 0.28, training loss: 2.0440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   74/1000, training accuracy: 0.16, training loss: 2.1819\niteration   76/1000, training accuracy: 0.16, training loss: 2.4002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   78/1000, training accuracy: 0.32, training loss: 1.9212\niteration   80/1000, training accuracy: 0.34, training loss: 1.8714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   82/1000, training accuracy: 0.33, training loss: 1.8688\niteration   84/1000, training accuracy: 0.33, training loss: 1.8987\niteration   86/1000, training accuracy: 0.32, training loss: 1.9703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   88/1000, training accuracy: 0.11, training loss: 2.2730\niteration   90/1000, training accuracy: 0.21, training loss: 2.1851\niteration   92/1000, training accuracy: 0.26, training loss: 1.9594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   94/1000, training accuracy: 0.26, training loss: 1.9257\niteration   96/1000, training accuracy: 0.26, training loss: 1.9170\niteration   98/1000, training accuracy: 0.26, training loss: 1.9110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  100/1000, training accuracy: 0.28, training loss: 1.9045\niteration  102/1000, training accuracy: 0.29, training loss: 1.8978\niteration  104/1000, training accuracy: 0.29, training loss: 1.8930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  106/1000, training accuracy: 0.29, training loss: 1.8809\niteration  108/1000, training accuracy: 0.31, training loss: 1.8647\niteration  110/1000, training accuracy: 0.32, training loss: 1.8365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  112/1000, training accuracy: 0.32, training loss: 1.8260\niteration  114/1000, training accuracy: 0.35, training loss: 1.7992\niteration  116/1000, training accuracy: 0.37, training loss: 1.7973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  118/1000, training accuracy: 0.34, training loss: 1.8216\niteration  120/1000, training accuracy: 0.34, training loss: 1.8413\niteration  122/1000, training accuracy: 0.31, training loss: 1.8456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  124/1000, training accuracy: 0.31, training loss: 1.8518\niteration  126/1000, training accuracy: 0.30, training loss: 1.8871\niteration  128/1000, training accuracy: 0.11, training loss: 2.4807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  130/1000, training accuracy: 0.15, training loss: 2.3987\niteration  132/1000, training accuracy: 0.24, training loss: 2.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  134/1000, training accuracy: 0.24, training loss: 1.9509\niteration  136/1000, training accuracy: 0.24, training loss: 1.9345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  138/1000, training accuracy: 0.26, training loss: 1.9233\niteration  140/1000, training accuracy: 0.26, training loss: 1.9208\niteration  142/1000, training accuracy: 0.26, training loss: 1.9021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  144/1000, training accuracy: 0.26, training loss: 1.8976\niteration  146/1000, training accuracy: 0.26, training loss: 1.8943\niteration  148/1000, training accuracy: 0.27, training loss: 1.8957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  150/1000, training accuracy: 0.27, training loss: 1.8973\niteration  152/1000, training accuracy: 0.26, training loss: 1.8998\niteration  154/1000, training accuracy: 0.26, training loss: 1.8973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  156/1000, training accuracy: 0.26, training loss: 1.8842\niteration  158/1000, training accuracy: 0.26, training loss: 1.8786\niteration  160/1000, training accuracy: 0.26, training loss: 1.8848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  162/1000, training accuracy: 0.26, training loss: 1.8725\niteration  164/1000, training accuracy: 0.26, training loss: 1.8844\niteration  166/1000, training accuracy: 0.26, training loss: 1.8913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  168/1000, training accuracy: 0.26, training loss: 1.8698\niteration  170/1000, training accuracy: 0.26, training loss: 1.8887\niteration  172/1000, training accuracy: 0.26, training loss: 1.8780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  174/1000, training accuracy: 0.26, training loss: 1.8866\niteration  176/1000, training accuracy: 0.26, training loss: 1.8645\niteration  178/1000, training accuracy: 0.27, training loss: 1.8585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  180/1000, training accuracy: 0.30, training loss: 1.8395\niteration  182/1000, training accuracy: 0.30, training loss: 1.8499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  184/1000, training accuracy: 0.35, training loss: 1.8437\niteration  186/1000, training accuracy: 0.37, training loss: 1.8735\niteration  188/1000, training accuracy: 0.28, training loss: 1.9033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  190/1000, training accuracy: 0.27, training loss: 1.8933\niteration  192/1000, training accuracy: 0.28, training loss: 1.8828\niteration  194/1000, training accuracy: 0.28, training loss: 1.8686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  196/1000, training accuracy: 0.28, training loss: 1.8552\niteration  198/1000, training accuracy: 0.28, training loss: 1.8446\niteration  200/1000, training accuracy: 0.27, training loss: 1.8565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  202/1000, training accuracy: 0.26, training loss: 1.9049\niteration  204/1000, training accuracy: 0.27, training loss: 1.8861\niteration  206/1000, training accuracy: 0.26, training loss: 1.8849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  208/1000, training accuracy: 0.26, training loss: 1.8860\niteration  210/1000, training accuracy: 0.27, training loss: 1.8883\niteration  212/1000, training accuracy: 0.27, training loss: 1.8812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  214/1000, training accuracy: 0.26, training loss: 1.9050\niteration  216/1000, training accuracy: 0.23, training loss: 1.8760\niteration  218/1000, training accuracy: 0.24, training loss: 1.8742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  220/1000, training accuracy: 0.24, training loss: 1.8601\niteration  222/1000, training accuracy: 0.26, training loss: 1.8442\niteration  224/1000, training accuracy: 0.29, training loss: 1.7758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  226/1000, training accuracy: 0.30, training loss: 1.8274\niteration  228/1000, training accuracy: 0.25, training loss: 1.9110\niteration  230/1000, training accuracy: 0.32, training loss: 1.8126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  232/1000, training accuracy: 0.29, training loss: 1.8903\niteration  234/1000, training accuracy: 0.29, training loss: 1.8624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  236/1000, training accuracy: 0.29, training loss: 1.8632\niteration  238/1000, training accuracy: 0.31, training loss: 1.8165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  240/1000, training accuracy: 0.30, training loss: 1.8887\niteration  242/1000, training accuracy: 0.29, training loss: 1.8887\niteration  244/1000, training accuracy: 0.31, training loss: 1.8529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  246/1000, training accuracy: 0.33, training loss: 1.8059\niteration  248/1000, training accuracy: 0.28, training loss: 1.9254\niteration  250/1000, training accuracy: 0.27, training loss: 1.9014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  252/1000, training accuracy: 0.30, training loss: 1.6842\niteration  254/1000, training accuracy: 0.28, training loss: 1.9093\niteration  256/1000, training accuracy: 0.27, training loss: 1.8572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  258/1000, training accuracy: 0.29, training loss: 1.6923\niteration  260/1000, training accuracy: 0.27, training loss: 1.9150\niteration  262/1000, training accuracy: 0.32, training loss: 1.6475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  264/1000, training accuracy: 0.35, training loss: 1.6364\niteration  266/1000, training accuracy: 0.30, training loss: 1.8341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  268/1000, training accuracy: 0.25, training loss: 1.9897\niteration  270/1000, training accuracy: 0.33, training loss: 1.6631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  272/1000, training accuracy: 0.32, training loss: 1.7937\niteration  274/1000, training accuracy: 0.33, training loss: 1.7386\niteration  276/1000, training accuracy: 0.31, training loss: 1.8288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  278/1000, training accuracy: 0.33, training loss: 1.7278\niteration  280/1000, training accuracy: 0.35, training loss: 1.6931\niteration  282/1000, training accuracy: 0.35, training loss: 1.7146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  284/1000, training accuracy: 0.32, training loss: 1.8460\niteration  286/1000, training accuracy: 0.26, training loss: 1.9399\niteration  288/1000, training accuracy: 0.35, training loss: 1.5926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  290/1000, training accuracy: 0.35, training loss: 1.6983\niteration  292/1000, training accuracy: 0.35, training loss: 1.7151\niteration  294/1000, training accuracy: 0.38, training loss: 1.4788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  296/1000, training accuracy: 0.35, training loss: 1.6418\niteration  298/1000, training accuracy: 0.34, training loss: 1.8047\niteration  300/1000, training accuracy: 0.33, training loss: 1.7961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  302/1000, training accuracy: 0.35, training loss: 1.6921\niteration  304/1000, training accuracy: 0.32, training loss: 1.7725\niteration  306/1000, training accuracy: 0.33, training loss: 1.7699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  308/1000, training accuracy: 0.32, training loss: 1.7470\niteration  310/1000, training accuracy: 0.32, training loss: 1.8759\niteration  312/1000, training accuracy: 0.31, training loss: 1.8345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  314/1000, training accuracy: 0.36, training loss: 1.5919\niteration  316/1000, training accuracy: 0.33, training loss: 1.6913\niteration  318/1000, training accuracy: 0.34, training loss: 1.7879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  320/1000, training accuracy: 0.36, training loss: 1.6440\niteration  322/1000, training accuracy: 0.41, training loss: 1.5992\niteration  324/1000, training accuracy: 0.32, training loss: 1.8159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  326/1000, training accuracy: 0.31, training loss: 1.9593\niteration  328/1000, training accuracy: 0.39, training loss: 1.4639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  330/1000, training accuracy: 0.47, training loss: 1.5682\niteration  332/1000, training accuracy: 0.36, training loss: 1.6977\niteration  334/1000, training accuracy: 0.32, training loss: 1.8561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  336/1000, training accuracy: 0.28, training loss: 2.0278\niteration  338/1000, training accuracy: 0.39, training loss: 1.6872\niteration  340/1000, training accuracy: 0.31, training loss: 1.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  342/1000, training accuracy: 0.40, training loss: 1.6350\niteration  344/1000, training accuracy: 0.38, training loss: 1.7485\niteration  346/1000, training accuracy: 0.36, training loss: 1.7594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  348/1000, training accuracy: 0.38, training loss: 1.7515\niteration  350/1000, training accuracy: 0.40, training loss: 1.7592\niteration  352/1000, training accuracy: 0.43, training loss: 1.7313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  354/1000, training accuracy: 0.36, training loss: 1.6675\niteration  356/1000, training accuracy: 0.36, training loss: 1.6735\niteration  358/1000, training accuracy: 0.35, training loss: 1.7343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  360/1000, training accuracy: 0.26, training loss: 2.1963\niteration  362/1000, training accuracy: 0.40, training loss: 1.6862\niteration  364/1000, training accuracy: 0.38, training loss: 1.6796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  366/1000, training accuracy: 0.38, training loss: 1.7106\niteration  368/1000, training accuracy: 0.38, training loss: 1.7539\niteration  370/1000, training accuracy: 0.39, training loss: 1.6706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  372/1000, training accuracy: 0.41, training loss: 1.5957\niteration  374/1000, training accuracy: 0.27, training loss: 1.7371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  376/1000, training accuracy: 0.43, training loss: 1.6256\niteration  378/1000, training accuracy: 0.35, training loss: 1.9074\niteration  380/1000, training accuracy: 0.30, training loss: 2.0760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  382/1000, training accuracy: 0.34, training loss: 1.8107\niteration  384/1000, training accuracy: 0.43, training loss: 1.5503\niteration  386/1000, training accuracy: 0.42, training loss: 1.5981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  388/1000, training accuracy: 0.33, training loss: 1.8686\niteration  390/1000, training accuracy: 0.42, training loss: 1.5803\niteration  392/1000, training accuracy: 0.36, training loss: 1.8945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  394/1000, training accuracy: 0.18, training loss: 2.1051\niteration  396/1000, training accuracy: 0.46, training loss: 1.4634\niteration  398/1000, training accuracy: 0.43, training loss: 1.5372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  400/1000, training accuracy: 0.32, training loss: 1.6911\niteration  402/1000, training accuracy: 0.43, training loss: 1.5597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  404/1000, training accuracy: 0.42, training loss: 1.6076\niteration  406/1000, training accuracy: 0.42, training loss: 1.6605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  408/1000, training accuracy: 0.37, training loss: 1.6482\niteration  410/1000, training accuracy: 0.31, training loss: 2.0156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  412/1000, training accuracy: 0.41, training loss: 1.5183\niteration  414/1000, training accuracy: 0.38, training loss: 1.7113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  416/1000, training accuracy: 0.28, training loss: 2.0995\niteration  418/1000, training accuracy: 0.42, training loss: 1.5379\niteration  420/1000, training accuracy: 0.41, training loss: 1.5805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  422/1000, training accuracy: 0.34, training loss: 1.7658\niteration  424/1000, training accuracy: 0.33, training loss: 1.9180\niteration  426/1000, training accuracy: 0.39, training loss: 1.6347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  428/1000, training accuracy: 0.42, training loss: 1.5527\niteration  430/1000, training accuracy: 0.31, training loss: 1.6907\niteration  432/1000, training accuracy: 0.37, training loss: 1.6915"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\niteration  434/1000, training accuracy: 0.47, training loss: 1.5324\niteration  436/1000, training accuracy: 0.22, training loss: 1.9541\niteration  438/1000, training accuracy: 0.41, training loss: 1.4696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  440/1000, training accuracy: 0.39, training loss: 1.5477\niteration  442/1000, training accuracy: 0.47, training loss: 1.5604\niteration  444/1000, training accuracy: 0.47, training loss: 1.5061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  446/1000, training accuracy: 0.51, training loss: 1.4479\niteration  448/1000, training accuracy: 0.32, training loss: 1.7977\niteration  450/1000, training accuracy: 0.41, training loss: 1.5111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  452/1000, training accuracy: 0.43, training loss: 1.5691\niteration  454/1000, training accuracy: 0.42, training loss: 1.5889\niteration  456/1000, training accuracy: 0.41, training loss: 1.5547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  458/1000, training accuracy: 0.39, training loss: 1.5759\niteration  460/1000, training accuracy: 0.37, training loss: 1.6129\niteration  462/1000, training accuracy: 0.33, training loss: 1.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  464/1000, training accuracy: 0.39, training loss: 1.5066\niteration  466/1000, training accuracy: 0.42, training loss: 1.6106\niteration  468/1000, training accuracy: 0.32, training loss: 1.7692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  470/1000, training accuracy: 0.40, training loss: 1.5181\niteration  472/1000, training accuracy: 0.45, training loss: 1.4802\niteration  474/1000, training accuracy: 0.36, training loss: 1.5134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  476/1000, training accuracy: 0.43, training loss: 1.6253\niteration  478/1000, training accuracy: 0.46, training loss: 1.4710\niteration  480/1000, training accuracy: 0.42, training loss: 1.5702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  482/1000, training accuracy: 0.32, training loss: 1.8767\niteration  484/1000, training accuracy: 0.40, training loss: 1.6583\niteration  486/1000, training accuracy: 0.51, training loss: 1.2988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  488/1000, training accuracy: 0.46, training loss: 1.4193\niteration  490/1000, training accuracy: 0.45, training loss: 1.4921\niteration  492/1000, training accuracy: 0.39, training loss: 1.6738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  494/1000, training accuracy: 0.32, training loss: 1.8548\niteration  496/1000, training accuracy: 0.50, training loss: 1.2819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  498/1000, training accuracy: 0.46, training loss: 1.4044\niteration  500/1000, training accuracy: 0.33, training loss: 1.6917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  502/1000, training accuracy: 0.46, training loss: 1.4717\niteration  504/1000, training accuracy: 0.42, training loss: 1.5272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  506/1000, training accuracy: 0.43, training loss: 1.5397\niteration  508/1000, training accuracy: 0.46, training loss: 1.4840\niteration  510/1000, training accuracy: 0.40, training loss: 1.5213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  512/1000, training accuracy: 0.40, training loss: 1.5272\niteration  514/1000, training accuracy: 0.46, training loss: 1.4947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  516/1000, training accuracy: 0.35, training loss: 1.6545\niteration  518/1000, training accuracy: 0.42, training loss: 1.4944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  520/1000, training accuracy: 0.40, training loss: 1.4951\niteration  522/1000, training accuracy: 0.41, training loss: 1.5828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  524/1000, training accuracy: 0.36, training loss: 1.8131\niteration  526/1000, training accuracy: 0.46, training loss: 1.5219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  528/1000, training accuracy: 0.40, training loss: 1.5636\niteration  530/1000, training accuracy: 0.43, training loss: 1.4586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  532/1000, training accuracy: 0.51, training loss: 1.3962\niteration  534/1000, training accuracy: 0.39, training loss: 1.5726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  536/1000, training accuracy: 0.32, training loss: 1.8060\niteration  538/1000, training accuracy: 0.44, training loss: 1.4496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  540/1000, training accuracy: 0.50, training loss: 1.3610\niteration  542/1000, training accuracy: 0.43, training loss: 1.4654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  544/1000, training accuracy: 0.46, training loss: 1.4658\niteration  546/1000, training accuracy: 0.24, training loss: 1.7952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  548/1000, training accuracy: 0.36, training loss: 1.7940\niteration  550/1000, training accuracy: 0.47, training loss: 1.3637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  552/1000, training accuracy: 0.48, training loss: 1.3755\niteration  554/1000, training accuracy: 0.45, training loss: 1.4203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  556/1000, training accuracy: 0.45, training loss: 1.4599\niteration  558/1000, training accuracy: 0.43, training loss: 1.5773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  560/1000, training accuracy: 0.36, training loss: 2.3075\niteration  562/1000, training accuracy: 0.48, training loss: 1.4391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  564/1000, training accuracy: 0.39, training loss: 1.4635\niteration  566/1000, training accuracy: 0.36, training loss: 1.5076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  568/1000, training accuracy: 0.40, training loss: 1.4771\niteration  570/1000, training accuracy: 0.37, training loss: 1.5418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  572/1000, training accuracy: 0.47, training loss: 1.4156\niteration  574/1000, training accuracy: 0.46, training loss: 1.4064\niteration  576/1000, training accuracy: 0.43, training loss: 1.4540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  578/1000, training accuracy: 0.40, training loss: 1.8580\niteration  580/1000, training accuracy: 0.41, training loss: 1.5157\niteration  582/1000, training accuracy: 0.47, training loss: 1.2950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  584/1000, training accuracy: 0.45, training loss: 1.3837\niteration  586/1000, training accuracy: 0.43, training loss: 1.4398\niteration  588/1000, training accuracy: 0.38, training loss: 1.6098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  590/1000, training accuracy: 0.28, training loss: 1.8270\niteration  592/1000, training accuracy: 0.43, training loss: 1.5849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  594/1000, training accuracy: 0.47, training loss: 1.3839\niteration  596/1000, training accuracy: 0.49, training loss: 1.4466\niteration  598/1000, training accuracy: 0.47, training loss: 1.4574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  600/1000, training accuracy: 0.45, training loss: 1.5467\niteration  602/1000, training accuracy: 0.36, training loss: 1.6689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  604/1000, training accuracy: 0.46, training loss: 1.4802\niteration  606/1000, training accuracy: 0.47, training loss: 1.4006\niteration  608/1000, training accuracy: 0.46, training loss: 1.4690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  610/1000, training accuracy: 0.37, training loss: 1.7718\niteration  612/1000, training accuracy: 0.46, training loss: 1.5195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  614/1000, training accuracy: 0.43, training loss: 1.7837\niteration  616/1000, training accuracy: 0.43, training loss: 1.9425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  618/1000, training accuracy: 0.49, training loss: 1.4957\niteration  620/1000, training accuracy: 0.47, training loss: 1.4685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  622/1000, training accuracy: 0.33, training loss: 1.8309\niteration  624/1000, training accuracy: 0.44, training loss: 1.4389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  626/1000, training accuracy: 0.46, training loss: 1.4378\niteration  628/1000, training accuracy: 0.34, training loss: 1.5818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  630/1000, training accuracy: 0.44, training loss: 1.6434\niteration  632/1000, training accuracy: 0.45, training loss: 1.4579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  634/1000, training accuracy: 0.36, training loss: 1.7802\niteration  636/1000, training accuracy: 0.42, training loss: 1.6780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  638/1000, training accuracy: 0.47, training loss: 1.3758\niteration  640/1000, training accuracy: 0.47, training loss: 1.3992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  642/1000, training accuracy: 0.42, training loss: 1.4389\niteration  644/1000, training accuracy: 0.39, training loss: 1.5694\niteration  646/1000, training accuracy: 0.45, training loss: 1.4546"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\niteration  648/1000, training accuracy: 0.43, training loss: 1.5150\niteration  650/1000, training accuracy: 0.47, training loss: 1.4374\niteration  652/1000, training accuracy: 0.44, training loss: 1.5267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  654/1000, training accuracy: 0.38, training loss: 1.5761\niteration  656/1000, training accuracy: 0.41, training loss: 2.0070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  658/1000, training accuracy: 0.48, training loss: 1.4924\niteration  660/1000, training accuracy: 0.46, training loss: 1.4201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  662/1000, training accuracy: 0.46, training loss: 1.4401\niteration  664/1000, training accuracy: 0.51, training loss: 1.3792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  666/1000, training accuracy: 0.28, training loss: 1.9024\niteration  668/1000, training accuracy: 0.32, training loss: 1.9164\niteration  670/1000, training accuracy: 0.49, training loss: 1.3719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  672/1000, training accuracy: 0.48, training loss: 1.3593\niteration  674/1000, training accuracy: 0.44, training loss: 1.4319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  676/1000, training accuracy: 0.45, training loss: 1.4394\niteration  678/1000, training accuracy: 0.48, training loss: 1.5795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  680/1000, training accuracy: 0.46, training loss: 1.4483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  682/1000, training accuracy: 0.46, training loss: 1.4020\niteration  684/1000, training accuracy: 0.39, training loss: 1.5522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  686/1000, training accuracy: 0.42, training loss: 1.4792\niteration  688/1000, training accuracy: 0.46, training loss: 1.3992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  690/1000, training accuracy: 0.40, training loss: 1.4703\niteration  692/1000, training accuracy: 0.38, training loss: 1.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  694/1000, training accuracy: 0.48, training loss: 1.3520\niteration  696/1000, training accuracy: 0.45, training loss: 1.4386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  698/1000, training accuracy: 0.45, training loss: 1.5047\niteration  700/1000, training accuracy: 0.42, training loss: 1.3765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  702/1000, training accuracy: 0.45, training loss: 1.4576\niteration  704/1000, training accuracy: 0.41, training loss: 1.4955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  706/1000, training accuracy: 0.45, training loss: 1.4590\niteration  708/1000, training accuracy: 0.50, training loss: 1.3020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  710/1000, training accuracy: 0.47, training loss: 1.3746\niteration  712/1000, training accuracy: 0.38, training loss: 1.6492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  714/1000, training accuracy: 0.41, training loss: 1.6477\niteration  716/1000, training accuracy: 0.53, training loss: 1.2851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  718/1000, training accuracy: 0.47, training loss: 1.3442\niteration  720/1000, training accuracy: 0.45, training loss: 1.3912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  722/1000, training accuracy: 0.44, training loss: 1.4510\niteration  724/1000, training accuracy: 0.24, training loss: 2.5982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  726/1000, training accuracy: 0.34, training loss: 1.7719\niteration  728/1000, training accuracy: 0.48, training loss: 1.4609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  730/1000, training accuracy: 0.45, training loss: 1.4251\niteration  732/1000, training accuracy: 0.44, training loss: 1.4637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  734/1000, training accuracy: 0.45, training loss: 1.4904\niteration  736/1000, training accuracy: 0.40, training loss: 1.5563\niteration  738/1000, training accuracy: 0.39, training loss: 1.7237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  740/1000, training accuracy: 0.45, training loss: 1.4771\niteration  742/1000, training accuracy: 0.46, training loss: 1.4804\niteration  744/1000, training accuracy: 0.44, training loss: 1.5604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  746/1000, training accuracy: 0.42, training loss: 1.6546\niteration  748/1000, training accuracy: 0.40, training loss: 1.9130\niteration  750/1000, training accuracy: 0.46, training loss: 1.4988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  752/1000, training accuracy: 0.44, training loss: 1.4904\niteration  754/1000, training accuracy: 0.44, training loss: 1.5003\niteration  756/1000, training accuracy: 0.44, training loss: 1.4412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  758/1000, training accuracy: 0.43, training loss: 1.4298\niteration  760/1000, training accuracy: 0.41, training loss: 1.4500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  762/1000, training accuracy: 0.45, training loss: 1.4470\niteration  764/1000, training accuracy: 0.45, training loss: 1.4924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  766/1000, training accuracy: 0.44, training loss: 1.3768\niteration  768/1000, training accuracy: 0.43, training loss: 1.3978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  770/1000, training accuracy: 0.42, training loss: 1.5681\niteration  772/1000, training accuracy: 0.47, training loss: 1.3632\niteration  774/1000, training accuracy: 0.44, training loss: 1.4022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  776/1000, training accuracy: 0.42, training loss: 1.5085\niteration  778/1000, training accuracy: 0.40, training loss: 1.6835\niteration  780/1000, training accuracy: 0.48, training loss: 1.3885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  782/1000, training accuracy: 0.45, training loss: 1.3631\niteration  784/1000, training accuracy: 0.45, training loss: 1.4076\niteration  786/1000, training accuracy: 0.43, training loss: 1.8960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  788/1000, training accuracy: 0.42, training loss: 1.4428\niteration  790/1000, training accuracy: 0.45, training loss: 1.3710\niteration  792/1000, training accuracy: 0.38, training loss: 1.4388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  794/1000, training accuracy: 0.33, training loss: 1.7079\niteration  796/1000, training accuracy: 0.44, training loss: 1.5190\niteration  798/1000, training accuracy: 0.49, training loss: 1.3856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  800/1000, training accuracy: 0.48, training loss: 1.4081\niteration  802/1000, training accuracy: 0.44, training loss: 1.4241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  804/1000, training accuracy: 0.44, training loss: 1.4946\niteration  806/1000, training accuracy: 0.44, training loss: 1.3695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  808/1000, training accuracy: 0.46, training loss: 1.3910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  810/1000, training accuracy: 0.45, training loss: 1.3424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  812/1000, training accuracy: 0.43, training loss: 1.4985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  814/1000, training accuracy: 0.41, training loss: 1.4469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  816/1000, training accuracy: 0.47, training loss: 1.4523\niteration  818/1000, training accuracy: 0.43, training loss: 1.5090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  820/1000, training accuracy: 0.49, training loss: 1.3092\niteration  822/1000, training accuracy: 0.48, training loss: 1.3377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  824/1000, training accuracy: 0.50, training loss: 1.3416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  826/1000, training accuracy: 0.45, training loss: 1.3786\niteration  828/1000, training accuracy: 0.39, training loss: 1.5981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  830/1000, training accuracy: 0.42, training loss: 1.3683\niteration  832/1000, training accuracy: 0.48, training loss: 1.3399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  834/1000, training accuracy: 0.47, training loss: 1.3437\niteration  836/1000, training accuracy: 0.48, training loss: 1.3573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  838/1000, training accuracy: 0.45, training loss: 1.3876\niteration  840/1000, training accuracy: 0.45, training loss: 1.4301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  842/1000, training accuracy: 0.48, training loss: 1.3308\niteration  844/1000, training accuracy: 0.47, training loss: 1.3389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  846/1000, training accuracy: 0.50, training loss: 1.3393\niteration  848/1000, training accuracy: 0.41, training loss: 1.4000\niteration  850/1000, training accuracy: 0.43, training loss: 1.5088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  852/1000, training accuracy: 0.48, training loss: 1.4509\niteration  854/1000, training accuracy: 0.48, training loss: 1.3530\niteration  856/1000, training accuracy: 0.48, training loss: 1.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  858/1000, training accuracy: 0.40, training loss: 1.4584\niteration  860/1000, training accuracy: 0.48, training loss: 1.3337\niteration  862/1000, training accuracy: 0.41, training loss: 1.5705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  864/1000, training accuracy: 0.42, training loss: 1.5238\niteration  866/1000, training accuracy: 0.47, training loss: 1.3288\niteration  868/1000, training accuracy: 0.49, training loss: 1.3083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  870/1000, training accuracy: 0.50, training loss: 1.3250\niteration  872/1000, training accuracy: 0.42, training loss: 1.4736\niteration  874/1000, training accuracy: 0.41, training loss: 1.3638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  876/1000, training accuracy: 0.47, training loss: 1.3874\niteration  878/1000, training accuracy: 0.48, training loss: 1.3955\niteration  880/1000, training accuracy: 0.50, training loss: 1.3268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  882/1000, training accuracy: 0.47, training loss: 1.3321\niteration  884/1000, training accuracy: 0.36, training loss: 1.5714\niteration  886/1000, training accuracy: 0.43, training loss: 1.3299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  888/1000, training accuracy: 0.49, training loss: 1.3097\niteration  890/1000, training accuracy: 0.49, training loss: 1.3283\niteration  892/1000, training accuracy: 0.40, training loss: 1.4568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  894/1000, training accuracy: 0.49, training loss: 1.3081\niteration  896/1000, training accuracy: 0.48, training loss: 1.3287\niteration  898/1000, training accuracy: 0.47, training loss: 1.4019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  900/1000, training accuracy: 0.49, training loss: 1.2952\niteration  902/1000, training accuracy: 0.48, training loss: 1.3097\niteration  904/1000, training accuracy: 0.38, training loss: 1.5277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  906/1000, training accuracy: 0.38, training loss: 1.4659\niteration  908/1000, training accuracy: 0.53, training loss: 1.2343\niteration  910/1000, training accuracy: 0.52, training loss: 1.2679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  912/1000, training accuracy: 0.46, training loss: 1.3117\niteration  914/1000, training accuracy: 0.42, training loss: 1.3707\niteration  916/1000, training accuracy: 0.42, training loss: 1.4686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  918/1000, training accuracy: 0.42, training loss: 1.5519\niteration  920/1000, training accuracy: 0.48, training loss: 1.2824\niteration  922/1000, training accuracy: 0.48, training loss: 1.3213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  924/1000, training accuracy: 0.44, training loss: 1.3549\niteration  926/1000, training accuracy: 0.42, training loss: 1.4541\niteration  928/1000, training accuracy: 0.42, training loss: 1.5844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  930/1000, training accuracy: 0.50, training loss: 1.2525\niteration  932/1000, training accuracy: 0.50, training loss: 1.3053\niteration  934/1000, training accuracy: 0.41, training loss: 1.3618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  936/1000, training accuracy: 0.40, training loss: 1.3816\niteration  938/1000, training accuracy: 0.37, training loss: 1.6607\niteration  940/1000, training accuracy: 0.42, training loss: 1.4098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  942/1000, training accuracy: 0.43, training loss: 1.3838\niteration  944/1000, training accuracy: 0.40, training loss: 1.6815\niteration  946/1000, training accuracy: 0.52, training loss: 1.2219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  948/1000, training accuracy: 0.51, training loss: 1.2779\niteration  950/1000, training accuracy: 0.44, training loss: 1.3904\niteration  952/1000, training accuracy: 0.49, training loss: 1.3523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  954/1000, training accuracy: 0.35, training loss: 1.6040\niteration  956/1000, training accuracy: 0.45, training loss: 1.2702\niteration  958/1000, training accuracy: 0.51, training loss: 1.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  960/1000, training accuracy: 0.42, training loss: 1.3914\niteration  962/1000, training accuracy: 0.41, training loss: 1.4533\niteration  964/1000, training accuracy: 0.51, training loss: 1.2439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  966/1000, training accuracy: 0.46, training loss: 1.3473\niteration  968/1000, training accuracy: 0.45, training loss: 1.4106\niteration  970/1000, training accuracy: 0.47, training loss: 1.3529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  972/1000, training accuracy: 0.46, training loss: 1.3279\niteration  974/1000, training accuracy: 0.46, training loss: 1.3877\niteration  976/1000, training accuracy: 0.42, training loss: 1.4528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  978/1000, training accuracy: 0.27, training loss: 1.9752\niteration  980/1000, training accuracy: 0.40, training loss: 1.8935\niteration  982/1000, training accuracy: 0.51, training loss: 1.4412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  984/1000, training accuracy: 0.41, training loss: 1.5135\niteration  986/1000, training accuracy: 0.45, training loss: 1.4771\niteration  988/1000, training accuracy: 0.42, training loss: 1.5973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  990/1000, training accuracy: 0.44, training loss: 1.4160\niteration  992/1000, training accuracy: 0.44, training loss: 1.4289\niteration  994/1000, training accuracy: 0.44, training loss: 1.4386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  996/1000, training accuracy: 0.41, training loss: 1.4806\niteration  998/1000, training accuracy: 0.40, training loss: 1.5482\n\nfinal training loss: 1.5008"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nfinal training accuracy: 0.32, best validation accuracy: 0.31\n\nfinal test accuracy: 0.33\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters, you should play around with these!:\n",
    "\n",
    "layer_sizes = [100, 50, 25]  # size per hidden layer\n",
    "\n",
    "num_epoch   = 100            # total number of epochs for training\n",
    "batch_size  = 100            # batch size of training samples per forward and backward pass\n",
    "lrn_rate    = 7e-3           # learning rate for stochastic gradient descent\n",
    "reg_str     = 0              # regularization strenght (L2 implemented)\n",
    "print_every = 2             # during training, print current performance every this amount of iterations\n",
    "\n",
    "# a Vanilla net takes a flat feature array as input instead of a 2D image, so change the shape of the images:\n",
    "tr_img_flat = tr_img.reshape(N_tr, -1)\n",
    "va_img_flat = va_img.reshape(N_va, -1)\n",
    "te_img_flat = te_img.reshape(N_te, -1)\n",
    "\n",
    "# initialise a Vanilla network class, please look in the Models.py file for its API and workings\n",
    "VanNet = Vanilla_NN(input_size       = (H*W),\n",
    "                    layer_sizes      = layer_sizes,\n",
    "                    output_size      = C)\n",
    "# train\n",
    "tr_loss_hist, va_loss_hist, tr_acc_hist, va_acc_hist = VanNet.train(tr_img_flat, tr_lbl, va_img_flat, va_lbl,\n",
    "                                                                    number_epochs = num_epoch,\n",
    "                                                                    batch_size    = batch_size,\n",
    "                                                                    learning_rate = lrn_rate,\n",
    "                                                                    reg_strength  = reg_str,\n",
    "                                                                    print_every   = print_every,\n",
    "                                                                    verbose       = True)\n",
    "\n",
    "print('\\nfinal training loss: %.4f' % (tr_loss_hist[-1]))\n",
    "print('final training accuracy: %.2f, best validation accuracy: %.2f' % (tr_acc_hist[-1], np.max(va_acc_hist)))\n",
    "\n",
    "# test\n",
    "loss, pred, final_test_accuracy = VanNet.test(te_img_flat, te_lbl)\n",
    "print('\\nfinal test accuracy: %.2f' % final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next up, the slightly fancier model, BN_Drop_NN!\n",
    "## NB this implementation is almost the same as Vanilla_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration    0/1000, training accuracy: 0.06, training loss: 2.3585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   50/1000, training accuracy: 0.09, training loss: 2.3658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  100/1000, training accuracy: 0.14, training loss: 2.3038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  150/1000, training accuracy: 0.14, training loss: 2.2956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  200/1000, training accuracy: 0.12, training loss: 2.3071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  250/1000, training accuracy: 0.06, training loss: 2.3117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  300/1000, training accuracy: 0.07, training loss: 2.3207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  350/1000, training accuracy: 0.17, training loss: 2.2809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  400/1000, training accuracy: 0.08, training loss: 2.3117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-6a5cafad5889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                                                                     \u001b[0mdrop_prob\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                                     \u001b[0mprint_every\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                                                                     verbose       = True)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nfinal training loss: %.4f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtr_loss_hist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anner\\Documents\\Werk\\LeapMind\\Models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_tr, y_tr, X_va, y_va, number_epochs, batch_size, learning_rate, reg_strength, drop_prob, print_every, verbose)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[1;31m# get current validation performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mva_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mva_acc\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_va\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_va\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[1;31m# forward and backpropagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anner\\Documents\\Werk\\LeapMind\\Models.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \"\"\"\n\u001b[1;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anner\\Documents\\Werk\\LeapMind\\Models.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, X, y, drop_prob)\u001b[0m\n\u001b[1;32m    317\u001b[0m             h, cur_cache           = BN_Dr_sig_layer_forward(inp, cur_W, cur_b,    # forward layer calculation\n\u001b[1;32m    318\u001b[0m                                                              \u001b[0mdrop_prob\u001b[0m\u001b[1;33m,\u001b[0m            \u001b[1;31m# dropout parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                                                              cur_gam, cur_bet, self.bn_params[i])   # batch normalization parameters\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0minp\u001b[0m                    \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'c%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anner\\Documents\\Werk\\LeapMind\\nn_utils.py\u001b[0m in \u001b[0;36mBN_Dr_sig_layer_forward\u001b[0;34m(inp, W, b, drop, gam, bet, bn_param)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[1;31m# implements vanilla foward neural net layer, then BN, then Dropout, then a sigmoid activation, assumes all input to be numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0mun_act\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc_cache\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0maffine_layer_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mBN\u001b[0m\u001b[1;33m,\u001b[0m     \u001b[0mBN_cache\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mbatch_norm_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mun_act\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m     \u001b[0mDrop\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[0mDr_cache\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mdropout_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[0msig_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anner\\Documents\\Werk\\LeapMind\\nn_utils.py\u001b[0m in \u001b[0;36mbatch_norm_forward\u001b[0;34m(inp, gamma, beta, bn_param)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0minp_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0minp_susq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0minp_sqsu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0minp_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp_susq\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0minp_sqsu\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# This implementation is almost the same\n",
    "# Hyperparameters, you should play around with these!:\n",
    "\n",
    "layer_sizes = [100, 50, 25]  # size per hidden layer\n",
    "\n",
    "num_epoch   = 100             # total number of epochs for training\n",
    "batch_size  = 100             # batch size of training samples per forward and backward pass\n",
    "lrn_rate    = 5e-5           # learning rate for stochastic gradient descent\n",
    "reg_str     = 0           # regularization strenght (L2 implemented)\n",
    "drop_prob   = 0            # the probability with which every neuron is dropped, should be in range [0-1) (NOT 1!)\n",
    "print_every = 50             # during training, print current performance every this amount of iterations\n",
    "\n",
    "# a Vanilla net takes a flat feature array as input instead of a 2D image, so change the shape of the images:\n",
    "tr_img_flat = tr_img.reshape(N_tr, -1)\n",
    "va_img_flat = va_img.reshape(N_va, -1)\n",
    "te_img_flat = te_img.reshape(N_te, -1)\n",
    "\n",
    "# initialise a Vanilla network class, please look in the Models.py file for its API and workings\n",
    "BN_Drop_Net = BN_Drop_NN(input_size       = (H*W),\n",
    "                         layer_sizes      = layer_sizes,\n",
    "                         output_size      = C)\n",
    "# train\n",
    "tr_loss_hist, va_loss_hist, tr_acc_hist, va_acc_hist = BN_Drop_Net.train(tr_img_flat, tr_lbl, va_img_flat, va_lbl,\n",
    "                                                                    number_epochs = num_epoch,\n",
    "                                                                    batch_size    = batch_size,\n",
    "                                                                    learning_rate = lrn_rate,\n",
    "                                                                    reg_strength  = reg_str,\n",
    "                                                                    drop_prob     = drop_prob,\n",
    "                                                                    print_every   = print_every,\n",
    "                                                                    verbose       = True)\n",
    "\n",
    "print('\\nfinal training loss: %.4f' % (tr_loss_hist[-1]))\n",
    "print('final training accuracy: %.2f, best validation accuracy: %.2f' % (tr_acc_hist[-1], np.max(va_acc_hist)))\n",
    "\n",
    "# test\n",
    "loss, pred, final_test_accuracy = BN_Drop_Net.test(te_img_flat, te_lbl)\n",
    "print('\\nfinal test accuracy: %.2f' % final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And finally, the Conv_NN!\n",
    "\n",
    "Some drawbacks:\n",
    "* The stride and padding of the implemented layers are fixed\n",
    "* Conv layers dont have batch normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration    0/10000, training accuracy: 0.12, training loss: 2.3293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   50/10000, training accuracy: 0.12, training loss: 2.3460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  100/10000, training accuracy: 0.08, training loss: 2.3178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  150/10000, training accuracy: 0.07, training loss: 2.3257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  200/10000, training accuracy: 0.05, training loss: 2.3296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  250/10000, training accuracy: 0.08, training loss: 2.2998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  300/10000, training accuracy: 0.13, training loss: 2.2896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  350/10000, training accuracy: 0.04, training loss: 2.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  400/10000, training accuracy: 0.06, training loss: 2.2979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  450/10000, training accuracy: 0.15, training loss: 2.2962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  500/10000, training accuracy: 0.11, training loss: 2.3062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  550/10000, training accuracy: 0.10, training loss: 2.2884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  600/10000, training accuracy: 0.16, training loss: 2.2833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  650/10000, training accuracy: 0.12, training loss: 2.3017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  700/10000, training accuracy: 0.12, training loss: 2.2932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  750/10000, training accuracy: 0.11, training loss: 2.2954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  800/10000, training accuracy: 0.14, training loss: 2.2894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  850/10000, training accuracy: 0.13, training loss: 2.2801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  900/10000, training accuracy: 0.14, training loss: 2.2856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  950/10000, training accuracy: 0.08, training loss: 2.3003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000/10000, training accuracy: 0.13, training loss: 2.2911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1050/10000, training accuracy: 0.11, training loss: 2.2952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100/10000, training accuracy: 0.19, training loss: 2.2762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1150/10000, training accuracy: 0.08, training loss: 2.3093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200/10000, training accuracy: 0.10, training loss: 2.2753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1250/10000, training accuracy: 0.07, training loss: 2.3027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300/10000, training accuracy: 0.14, training loss: 2.2892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1350/10000, training accuracy: 0.16, training loss: 2.3012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400/10000, training accuracy: 0.10, training loss: 2.3223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1450/10000, training accuracy: 0.09, training loss: 2.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1500/10000, training accuracy: 0.15, training loss: 2.2952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1550/10000, training accuracy: 0.13, training loss: 2.2977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1600/10000, training accuracy: 0.06, training loss: 2.3160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1650/10000, training accuracy: 0.15, training loss: 2.2895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1700/10000, training accuracy: 0.08, training loss: 2.3264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1750/10000, training accuracy: 0.11, training loss: 2.3130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1800/10000, training accuracy: 0.06, training loss: 2.3083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1850/10000, training accuracy: 0.10, training loss: 2.2909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1900/10000, training accuracy: 0.08, training loss: 2.2972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1950/10000, training accuracy: 0.10, training loss: 2.2982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2000/10000, training accuracy: 0.15, training loss: 2.2962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2050/10000, training accuracy: 0.07, training loss: 2.3263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2100/10000, training accuracy: 0.11, training loss: 2.3007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2150/10000, training accuracy: 0.11, training loss: 2.2676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2200/10000, training accuracy: 0.11, training loss: 2.3057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2250/10000, training accuracy: 0.10, training loss: 2.3117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2300/10000, training accuracy: 0.14, training loss: 2.2871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2350/10000, training accuracy: 0.11, training loss: 2.3112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2400/10000, training accuracy: 0.09, training loss: 2.2961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2450/10000, training accuracy: 0.13, training loss: 2.2921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2500/10000, training accuracy: 0.13, training loss: 2.2784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2550/10000, training accuracy: 0.14, training loss: 2.3171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2600/10000, training accuracy: 0.15, training loss: 2.2851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2650/10000, training accuracy: 0.14, training loss: 2.2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2700/10000, training accuracy: 0.14, training loss: 2.2823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2750/10000, training accuracy: 0.13, training loss: 2.2785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2800/10000, training accuracy: 0.03, training loss: 2.3037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2850/10000, training accuracy: 0.15, training loss: 2.2868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2900/10000, training accuracy: 0.10, training loss: 2.2998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2950/10000, training accuracy: 0.10, training loss: 2.2864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3000/10000, training accuracy: 0.11, training loss: 2.3021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3050/10000, training accuracy: 0.12, training loss: 2.2912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3100/10000, training accuracy: 0.13, training loss: 2.2921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3150/10000, training accuracy: 0.15, training loss: 2.2939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3200/10000, training accuracy: 0.09, training loss: 2.3110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3250/10000, training accuracy: 0.10, training loss: 2.2916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3300/10000, training accuracy: 0.13, training loss: 2.2865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3350/10000, training accuracy: 0.15, training loss: 2.2938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3400/10000, training accuracy: 0.08, training loss: 2.3110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3450/10000, training accuracy: 0.14, training loss: 2.2879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3500/10000, training accuracy: 0.13, training loss: 2.2839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3550/10000, training accuracy: 0.15, training loss: 2.3046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3600/10000, training accuracy: 0.11, training loss: 2.2812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3650/10000, training accuracy: 0.12, training loss: 2.2971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3700/10000, training accuracy: 0.11, training loss: 2.3061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3750/10000, training accuracy: 0.09, training loss: 2.3061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3800/10000, training accuracy: 0.12, training loss: 2.3059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3850/10000, training accuracy: 0.17, training loss: 2.2932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3900/10000, training accuracy: 0.17, training loss: 2.2839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3950/10000, training accuracy: 0.12, training loss: 2.2857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4000/10000, training accuracy: 0.12, training loss: 2.2806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4050/10000, training accuracy: 0.10, training loss: 2.3064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4100/10000, training accuracy: 0.21, training loss: 2.2748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4150/10000, training accuracy: 0.11, training loss: 2.2919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4200/10000, training accuracy: 0.13, training loss: 2.2930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4250/10000, training accuracy: 0.11, training loss: 2.3038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4300/10000, training accuracy: 0.11, training loss: 2.2854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4350/10000, training accuracy: 0.12, training loss: 2.3054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4400/10000, training accuracy: 0.16, training loss: 2.2879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4450/10000, training accuracy: 0.08, training loss: 2.2945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4500/10000, training accuracy: 0.12, training loss: 2.2843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4550/10000, training accuracy: 0.12, training loss: 2.2847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4600/10000, training accuracy: 0.17, training loss: 2.2912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4650/10000, training accuracy: 0.12, training loss: 2.2839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4700/10000, training accuracy: 0.13, training loss: 2.2772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4750/10000, training accuracy: 0.09, training loss: 2.3137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4800/10000, training accuracy: 0.14, training loss: 2.2930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4850/10000, training accuracy: 0.13, training loss: 2.2923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4900/10000, training accuracy: 0.08, training loss: 2.2962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4950/10000, training accuracy: 0.13, training loss: 2.2994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5000/10000, training accuracy: 0.17, training loss: 2.2944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5050/10000, training accuracy: 0.15, training loss: 2.2738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5100/10000, training accuracy: 0.15, training loss: 2.2952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5150/10000, training accuracy: 0.10, training loss: 2.2881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5200/10000, training accuracy: 0.12, training loss: 2.3123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5250/10000, training accuracy: 0.12, training loss: 2.2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5300/10000, training accuracy: 0.12, training loss: 2.2813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5350/10000, training accuracy: 0.10, training loss: 2.2931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5400/10000, training accuracy: 0.06, training loss: 2.2875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5450/10000, training accuracy: 0.06, training loss: 2.2920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5500/10000, training accuracy: 0.13, training loss: 2.2872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5550/10000, training accuracy: 0.05, training loss: 2.3146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5600/10000, training accuracy: 0.14, training loss: 2.2759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5650/10000, training accuracy: 0.10, training loss: 2.3099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5700/10000, training accuracy: 0.11, training loss: 2.2785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5750/10000, training accuracy: 0.08, training loss: 2.2943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5800/10000, training accuracy: 0.13, training loss: 2.2933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5850/10000, training accuracy: 0.13, training loss: 2.2963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5900/10000, training accuracy: 0.13, training loss: 2.2959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5950/10000, training accuracy: 0.10, training loss: 2.2916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6000/10000, training accuracy: 0.16, training loss: 2.2774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6050/10000, training accuracy: 0.09, training loss: 2.2994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6100/10000, training accuracy: 0.06, training loss: 2.3091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6150/10000, training accuracy: 0.09, training loss: 2.3025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6200/10000, training accuracy: 0.15, training loss: 2.2804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6250/10000, training accuracy: 0.15, training loss: 2.2909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6300/10000, training accuracy: 0.15, training loss: 2.3136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6350/10000, training accuracy: 0.09, training loss: 2.3045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6400/10000, training accuracy: 0.11, training loss: 2.2901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6450/10000, training accuracy: 0.08, training loss: 2.3148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6500/10000, training accuracy: 0.14, training loss: 2.2860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6550/10000, training accuracy: 0.09, training loss: 2.2926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6600/10000, training accuracy: 0.11, training loss: 2.2932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6650/10000, training accuracy: 0.09, training loss: 2.2889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6700/10000, training accuracy: 0.12, training loss: 2.2908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6750/10000, training accuracy: 0.12, training loss: 2.2780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6800/10000, training accuracy: 0.09, training loss: 2.3065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6850/10000, training accuracy: 0.08, training loss: 2.2936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6900/10000, training accuracy: 0.08, training loss: 2.2978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6950/10000, training accuracy: 0.07, training loss: 2.3112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7000/10000, training accuracy: 0.14, training loss: 2.2923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7050/10000, training accuracy: 0.13, training loss: 2.3008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7100/10000, training accuracy: 0.13, training loss: 2.2802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7150/10000, training accuracy: 0.10, training loss: 2.3139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7200/10000, training accuracy: 0.09, training loss: 2.3067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7250/10000, training accuracy: 0.16, training loss: 2.2801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7300/10000, training accuracy: 0.12, training loss: 2.2844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7350/10000, training accuracy: 0.07, training loss: 2.2825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7400/10000, training accuracy: 0.12, training loss: 2.2786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7450/10000, training accuracy: 0.14, training loss: 2.2785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7500/10000, training accuracy: 0.10, training loss: 2.3006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7550/10000, training accuracy: 0.12, training loss: 2.2815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7600/10000, training accuracy: 0.11, training loss: 2.3103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7650/10000, training accuracy: 0.11, training loss: 2.2817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7700/10000, training accuracy: 0.11, training loss: 2.2837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7750/10000, training accuracy: 0.14, training loss: 2.3132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7800/10000, training accuracy: 0.09, training loss: 2.3023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7850/10000, training accuracy: 0.16, training loss: 2.2937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7900/10000, training accuracy: 0.19, training loss: 2.2859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7950/10000, training accuracy: 0.11, training loss: 2.2921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8000/10000, training accuracy: 0.13, training loss: 2.3001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8050/10000, training accuracy: 0.07, training loss: 2.3134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8100/10000, training accuracy: 0.15, training loss: 2.2898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8150/10000, training accuracy: 0.14, training loss: 2.2974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8200/10000, training accuracy: 0.13, training loss: 2.2941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8250/10000, training accuracy: 0.11, training loss: 2.3033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8300/10000, training accuracy: 0.14, training loss: 2.3062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8350/10000, training accuracy: 0.08, training loss: 2.2601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8400/10000, training accuracy: 0.11, training loss: 2.2940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8450/10000, training accuracy: 0.13, training loss: 2.2697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8500/10000, training accuracy: 0.15, training loss: 2.2762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8550/10000, training accuracy: 0.21, training loss: 2.2871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8600/10000, training accuracy: 0.15, training loss: 2.2910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8650/10000, training accuracy: 0.13, training loss: 2.3149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8700/10000, training accuracy: 0.08, training loss: 2.3033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8750/10000, training accuracy: 0.09, training loss: 2.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8800/10000, training accuracy: 0.15, training loss: 2.2898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8850/10000, training accuracy: 0.11, training loss: 2.3002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8900/10000, training accuracy: 0.11, training loss: 2.2947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8950/10000, training accuracy: 0.09, training loss: 2.3029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9000/10000, training accuracy: 0.14, training loss: 2.2995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9050/10000, training accuracy: 0.12, training loss: 2.2805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9100/10000, training accuracy: 0.13, training loss: 2.2863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9150/10000, training accuracy: 0.10, training loss: 2.2921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9200/10000, training accuracy: 0.14, training loss: 2.2832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9250/10000, training accuracy: 0.17, training loss: 2.2970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9300/10000, training accuracy: 0.12, training loss: 2.2842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9350/10000, training accuracy: 0.08, training loss: 2.3028\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-ee1283cd1ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                                                                     \u001b[0mdrop_prob\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                                                                     \u001b[0mprint_every\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                                                                     verbose       = True)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nfinal training loss: %.4f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtr_loss_hist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anner\\Documents\\Werk\\LeapMind\\Models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_tr, y_tr, X_va, y_va, number_epochs, batch_size, learning_rate, reg_strength, drop_prob, print_every, verbose)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[1;31m# get current validation performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0mva_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mva_acc\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_va\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_va\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[1;31m# forward and backpropagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anner\\Documents\\Werk\\LeapMind\\Models.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \"\"\"\n\u001b[1;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anner\\Documents\\Werk\\LeapMind\\Models.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, X, y, drop_prob)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0mcur_W\u001b[0m                  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'W%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mcur_b\u001b[0m                  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'b%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_cache\u001b[0m           \u001b[1;33m=\u001b[0m \u001b[0msig_conv_layer_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_b\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# forward layer calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0minp\u001b[0m                    \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anner\\Documents\\Werk\\LeapMind\\nn_utils.py\u001b[0m in \u001b[0;36msig_conv_layer_forward\u001b[0;34m(inp, W, b)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msig_conv_layer_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[1;31m# implements convolutional foward neural net layer with sigmoid activation, assumes all input to be numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0mun_act\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_cache\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mconv_layer_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[0msig_cache\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0msigmoid_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mun_act\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconv_cache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anner\\Documents\\Werk\\LeapMind\\nn_utils.py\u001b[0m in \u001b[0;36mconv_layer_forward\u001b[0;34m(inp, W, b)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[1;31m# output column    out   (N, F, oH, oW)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mout_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ijk,lj->ilk'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_col\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Anner\\.conda\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\einsumfunc.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[1;31m# If no optimization, run pure einsum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moptimize_arg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0mvalid_einsum_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'out'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dtype'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'order'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'casting'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Hyperparameters, you should play around with these!:\n",
    "\n",
    "conv_layers   = [5, 10]   # kernel output size of each convolutional layer\n",
    "                               #     (hint: len(conv_layers) also specifies the amount of convolutional layers)\n",
    "conv_window   = 3              # window size for conv weights, has to be 3, 5 or 7\n",
    "\n",
    "affine_layers = [100, 50, 25]  # size per hidden affine layers, stacked after conv layers\n",
    "\n",
    "num_epoch     = 1000            # total number of epochs for training\n",
    "batch_size    = 100            # batch size of training samples per forward and backward pass\n",
    "lrn_rate      = 5e-5           # learning rate for stochastic gradient descent\n",
    "reg_str       = 0              # regularization strenght (L2 implemented)\n",
    "drop_prob     = 0            # the probability with which every neuron is dropped, should be in range [0-1) (NOT 1!)\n",
    "print_every   = 50              # during training, print current performance every this amount of iterations\n",
    "\n",
    "# a Convolutional net takes a batch amount of 3D images as input (so a 4D array)\n",
    "# we need to add the color dimension, cause the black and white MNIST data set normally doesnt have that dimension:\n",
    "tr_img_3D = tr_img[:, None]\n",
    "va_img_3D = va_img[:, None]\n",
    "te_img_3D = te_img[:, None]\n",
    "\n",
    "# initialise a Vanilla network class, please look in the Models.py file for its API and workings\n",
    "Conv_Net = Conv_NN(input_dim          = (H, W, 1),\n",
    "                   conv_layer_sizes   = conv_layers,\n",
    "                   conv_window_size   = conv_window,\n",
    "                   affine_layer_sizes = affine_layers,\n",
    "                   output_size        = C)\n",
    "# train\n",
    "tr_loss_hist, va_loss_hist, tr_acc_hist, va_acc_hist = Conv_Net.train(tr_img_3D, tr_lbl, va_img_3D, va_lbl,\n",
    "                                                                    number_epochs = num_epoch,\n",
    "                                                                    batch_size    = batch_size,\n",
    "                                                                    learning_rate = lrn_rate,\n",
    "                                                                    reg_strength  = reg_str,\n",
    "                                                                    drop_prob     = drop_prob,\n",
    "                                                                    print_every   = print_every,\n",
    "                                                                    verbose       = True)\n",
    "\n",
    "print('\\nfinal training loss: %.4f' % (tr_loss_hist[-1]))\n",
    "print('final training accuracy: %.2f, best validation accuracy: %.2f' % (tr_acc_hist[-1], np.max(va_acc_hist)))\n",
    "\n",
    "# test\n",
    "loss, pred, final_test_accuracy = Conv_Net.test(te_img_3D, te_lbl)\n",
    "print('\\nfinal test accuracy: %.2f' % final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise the training of your last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot inside the ipython notebook\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "\n",
    "# plot training set and validation set loss histories\n",
    "plt.subplot(2, 1, 1)\n",
    "ep_axis = np.arange(len(tr_loss_hist)) * 1.0 / len(tr_loss_hist) * num_epoch\n",
    "plt.title('Training loss (softmax loss without regularisatioin loss)[-]')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(ep_axis, tr_loss_hist, label='training set')\n",
    "plt.plot(ep_axis, va_loss_hist, label='validation set')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# plot training set and validation set accuracies\n",
    "plt.subplot(2, 1, 2)\n",
    "ep_axis = np.arange(len(tr_acc_hist)) * 1.0 / len(tr_acc_hist) * num_epoch\n",
    "plt.title('Training & Validation accuracy (%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(ep_axis, tr_acc_hist, label='train')\n",
    "plt.plot(ep_axis, va_acc_hist, label='validation')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Discussion\n",
    "After playing around with the model a bit, a few key characteristics are interesting to notice:\n",
    "\n",
    "#### robustness\n",
    "The model is quite fragile, and only seldom has smooth loss and accuracy curves. This is likely caused by\n",
    "1. the dataset size (especially validation curves)\n",
    "2. the model is very simple, not including methods that increase robustness such as batch normalisation, dropout, adagrad (or other) variable optimizer. 'Internet' also says tanh activation functions work better on the iris training set\n",
    "\n",
    "Due to the scope of this assignment, none of these were implemented.\n",
    "Different amount of layers and layer sizes were tried, but everytime this requires finetuning of the hyperparameters again. A simple 3 layers with descending size is adopted in current saved output.\n",
    "\n",
    "#### epochs\n",
    "Normally I'd say more than 100 epoch is way too much, and will very likely incur overfitting. However, for the current dataset, probably because of its small size, this is not the case. It even needs up to around 400 epochs sometimes for the accuracy to finally fly up. The long flat line before might very well be caused by bad initialisation of the variables. Better would be for example xavier initialisation. Again due to the scope of this assignment this wasn't implemented. Another weird phenomena is the overshoot in accuracy that permanently drops down afterwards. \n",
    "\n",
    "#### regularisation\n",
    "L2 regularisation is used. This should reduce overfitting, and drag the validation loss down closer to the training loss. Most of the runs, overfitting can indeed be seen. It seems however that a bigger regularisation kills the training capacity and flattens the loss from the beginning (probably related to the above 'robustness'). 1e-3 already does this, so as a final value 5e-4 was chosen, even though this value does not always close the gap between training set and validation set performance.\n",
    "\n",
    "#### learning rate\n",
    "In line with the regularization and robustness, the model is very sensetive to the learning rate. For the current setting finally a value of 7e-4 was chosen. Perhaps a degrading learning rate (again, not implemented) would help the robustness as well.\n",
    "\n",
    "\n",
    "#### final test accuracy\n",
    "During training the model keeps a deepcopy of the best validation set performing variable values around. at the end the models variables are swapped for these best performing ones. (Arguing from the smoothness of the curves it might be better to select the variables of the best validation loss iteration instead) Normally the final values should be very close to the optimal one, or even be the optimal itself. However, due to all the above here, in this setup it makes a huge difference, and most of the times increases the final test set accuracy by 10-30 %. I believe it is not 'wrong' to use such a trick, although this is not best practice and is highly prone to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}